//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34385749
// Cuda compilation tools, release 12.5, V12.5.82
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_61
.address_size 64

	// .globl	cast_bf16_f32

.visible .entry cast_bf16_f32(
	.param .u64 cast_bf16_f32_param_0,
	.param .u64 cast_bf16_f32_param_1,
	.param .u64 cast_bf16_f32_param_2,
	.param .u64 cast_bf16_f32_param_3,
	.param .u64 cast_bf16_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_bf16_f32_param_0];
	ld.param.u64 	%rd25, [cast_bf16_f32_param_1];
	ld.param.u64 	%rd26, [cast_bf16_f32_param_2];
	ld.param.u64 	%rd27, [cast_bf16_f32_param_3];
	ld.param.u64 	%rd28, [cast_bf16_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB0_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB0_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB0_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB0_5;

$L__BB0_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB0_2;

$L__BB0_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_6;

$L__BB0_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB0_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB0_17:
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs3, [%rd56];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f32 	[%rd58], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB0_17;
	bra.uni 	$L__BB0_18;

$L__BB0_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB0_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB0_14;

$L__BB0_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB0_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB0_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB0_12;

$L__BB0_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB0_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB0_9;

	mul.wide.u32 	%rd49, %r38, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	shl.b64 	%rd51, %rd60, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_18;

$L__BB0_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	shl.b64 	%rd53, %rd60, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB0_14;

$L__BB0_18:
	ret;

}
	// .globl	cast_f32_bf16
.visible .entry cast_f32_bf16(
	.param .u64 cast_f32_bf16_param_0,
	.param .u64 cast_f32_bf16_param_1,
	.param .u64 cast_f32_bf16_param_2,
	.param .u64 cast_f32_bf16_param_3,
	.param .u64 cast_f32_bf16_param_4
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<19>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f32_bf16_param_0];
	ld.param.u64 	%rd25, [cast_f32_bf16_param_1];
	ld.param.u64 	%rd26, [cast_f32_bf16_param_2];
	ld.param.u64 	%rd27, [cast_f32_bf16_param_3];
	ld.param.u64 	%rd28, [cast_f32_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p37, %p2;
	@%p5 bra 	$L__BB1_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r49, 0;

$L__BB1_2:
	not.b32 	%r23, %r49;
	cvt.u64.u32 	%rd30, %r23;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB1_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p37, 0;
	@%p8 bra 	$L__BB1_5;

$L__BB1_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd37, %r49;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p37, %p2;
	@%p10 bra 	$L__BB1_2;

$L__BB1_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r50, %r24, %r3, %r25;
	cvt.u64.u32 	%rd60, %r50;
	@%p37 bra 	$L__BB1_21;
	bra.uni 	$L__BB1_6;

$L__BB1_21:
	setp.ge.u64 	%p29, %rd60, %rd24;
	@%p29 bra 	$L__BB1_27;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r44;

$L__BB1_23:
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r45, [%rd56];
	and.b32  	%r46, %r45, 2147483647;
	setp.gt.u32 	%p30, %r46, 2139095040;
	shl.b32 	%r47, %r45, 16;
	shr.u32 	%r48, %r45, 16;
	cvt.u16.u32 	%rs14, %r48;
	selp.b32 	%r20, 0, %r47, %p30;
	selp.b16 	%rs18, 32767, %rs14, %p30;
	setp.gt.u32 	%p31, %r20, -2147483648;
	@%p31 bra 	$L__BB1_25;

	setp.ne.s32 	%p32, %r20, -2147483648;
	and.b16  	%rs15, %rs18, 1;
	setp.eq.b16 	%p33, %rs15, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB1_26;

$L__BB1_25:
	add.s16 	%rs18, %rs18, 1;

$L__BB1_26:
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs18;
	add.s32 	%r50, %r50, %r18;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p36, %rd60, %rd24;
	@%p36 bra 	$L__BB1_23;
	bra.uni 	$L__BB1_27;

$L__BB1_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB1_27;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r26;
	@%p3 bra 	$L__BB1_17;

$L__BB1_8:
	mov.u32 	%r51, 0;
	mov.u32 	%r52, %r50;
	mov.u32 	%r53, %r51;

$L__BB1_9:
	not.b32 	%r29, %r51;
	cvt.u64.u32 	%rd38, %r29;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r52;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB1_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB1_12;

$L__BB1_11:
	cvt.u32.u64 	%r30, %rd12;
	cvt.u32.u64 	%r31, %rd10;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd61, %r32;
	cvt.u64.u32 	%rd62, %r34;

$L__BB1_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r35, %rd47;
	add.s32 	%r53, %r53, %r35;
	cvt.u32.u64 	%r52, %rd61;
	add.s32 	%r51, %r51, 1;
	cvt.u64.u32 	%rd48, %r51;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB1_9;

	mul.wide.u32 	%rd49, %r53, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r36, [%rd50];
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p15, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs10, %r39;
	selp.b32 	%r13, 0, %r38, %p15;
	selp.b16 	%rs16, 32767, %rs10, %p15;
	setp.gt.u32 	%p16, %r13, -2147483648;
	@%p16 bra 	$L__BB1_15;

	setp.ne.s32 	%p17, %r13, -2147483648;
	and.b16  	%rs11, %rs16, 1;
	setp.eq.b16 	%p18, %rs11, 1;
	not.pred 	%p19, %p18;
	or.pred  	%p20, %p17, %p19;
	@%p20 bra 	$L__BB1_16;

$L__BB1_15:
	add.s16 	%rs16, %rs16, 1;

$L__BB1_16:
	shl.b64 	%rd51, %rd60, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs16;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p21, %rd60, %rd24;
	@%p21 bra 	$L__BB1_8;
	bra.uni 	$L__BB1_27;

$L__BB1_17:
	ld.global.u32 	%r40, [%rd2];
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs12, %r43;
	selp.b32 	%r16, 0, %r42, %p22;
	selp.b16 	%rs17, 32767, %rs12, %p22;
	setp.gt.u32 	%p23, %r16, -2147483648;
	@%p23 bra 	$L__BB1_19;

	setp.ne.s32 	%p24, %r16, -2147483648;
	and.b16  	%rs13, %rs17, 1;
	setp.eq.b16 	%p25, %rs13, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB1_20;

$L__BB1_19:
	add.s16 	%rs17, %rs17, 1;

$L__BB1_20:
	shl.b64 	%rd53, %rd60, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs17;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p28, %rd60, %rd24;
	@%p28 bra 	$L__BB1_17;

$L__BB1_27:
	ret;

}
	// .globl	cast_bf16_u8
.visible .entry cast_bf16_u8(
	.param .u64 cast_bf16_u8_param_0,
	.param .u64 cast_bf16_u8_param_1,
	.param .u64 cast_bf16_u8_param_2,
	.param .u64 cast_bf16_u8_param_3,
	.param .u64 cast_bf16_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd24, [cast_bf16_u8_param_0];
	ld.param.u64 	%rd25, [cast_bf16_u8_param_1];
	ld.param.u64 	%rd26, [cast_bf16_u8_param_2];
	ld.param.u64 	%rd27, [cast_bf16_u8_param_3];
	ld.param.u64 	%rd28, [cast_bf16_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB2_5;

	mov.u64 	%rd56, 1;
	mov.u32 	%r37, 0;

$L__BB2_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB2_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd56, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB2_5;

$L__BB2_4:
	mul.lo.s64 	%rd56, %rd6, %rd56;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB2_2;

$L__BB2_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r38;
	@%p19 bra 	$L__BB2_15;
	bra.uni 	$L__BB2_6;

$L__BB2_15:
	setp.ge.u64 	%p17, %rd57, %rd24;
	@%p17 bra 	$L__BB2_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB2_17:
	shl.b64 	%rd53, %rd57, 1;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u16 	%rs3, [%rd54];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p18, %rd57, %rd24;
	@%p18 bra 	$L__BB2_17;
	bra.uni 	$L__BB2_18;

$L__BB2_6:
	setp.ge.u64 	%p11, %rd57, %rd24;
	@%p11 bra 	$L__BB2_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB2_14;

$L__BB2_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB2_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB2_11;

	div.u64 	%rd58, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd58, %rd12;
	sub.s64 	%rd59, %rd10, %rd43;
	bra.uni 	$L__BB2_12;

$L__BB2_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd58, %r29;
	cvt.u64.u32 	%rd59, %r31;

$L__BB2_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd59;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB2_9;

	mul.wide.u32 	%rd49, %r41, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvt.rzi.u32.f32 	%r33, %f1;
	add.s64 	%rd51, %rd1, %rd57;
	st.global.u8 	[%rd51], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p15, %rd57, %rd24;
	@%p15 bra 	$L__BB2_8;
	bra.uni 	$L__BB2_18;

$L__BB2_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	cvt.rzi.u32.f32 	%r34, %f2;
	add.s64 	%rd52, %rd1, %rd57;
	st.global.u8 	[%rd52], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p16, %rd57, %rd24;
	@%p16 bra 	$L__BB2_14;

$L__BB2_18:
	ret;

}
	// .globl	cast_bf16_f16
.visible .entry cast_bf16_f16(
	.param .u64 cast_bf16_f16_param_0,
	.param .u64 cast_bf16_f16_param_1,
	.param .u64 cast_bf16_f16_param_2,
	.param .u64 cast_bf16_f16_param_3,
	.param .u64 cast_bf16_f16_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_bf16_f16_param_0];
	ld.param.u64 	%rd25, [cast_bf16_f16_param_1];
	ld.param.u64 	%rd26, [cast_bf16_f16_param_2];
	ld.param.u64 	%rd27, [cast_bf16_f16_param_3];
	ld.param.u64 	%rd28, [cast_bf16_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB3_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB3_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB3_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB3_5;

$L__BB3_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB3_2;

$L__BB3_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB3_15;
	bra.uni 	$L__BB3_6;

$L__BB3_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB3_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB3_17:
	shl.b64 	%rd55, %rd59, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs5, [%rd56];
	// begin inline asm
	{ mov.b32 %f5, {0,%rs5};}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f5;}

	// end inline asm
	add.s64 	%rd57, %rd1, %rd55;
	st.global.u16 	[%rd57], %rs6;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB3_17;
	bra.uni 	$L__BB3_18;

$L__BB3_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB3_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB3_14;

$L__BB3_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB3_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB3_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB3_12;

$L__BB3_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB3_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB3_9;

	mul.wide.u32 	%rd49, %r38, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// end inline asm
	shl.b64 	%rd51, %rd59, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB3_8;
	bra.uni 	$L__BB3_18;

$L__BB3_14:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f3;}

	// end inline asm
	shl.b64 	%rd53, %rd59, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs4;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB3_14;

$L__BB3_18:
	ret;

}
	// .globl	cast_bf16_f64
.visible .entry cast_bf16_f64(
	.param .u64 cast_bf16_f64_param_0,
	.param .u64 cast_bf16_f64_param_1,
	.param .u64 cast_bf16_f64_param_2,
	.param .u64 cast_bf16_f64_param_3,
	.param .u64 cast_bf16_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_bf16_f64_param_0];
	ld.param.u64 	%rd25, [cast_bf16_f64_param_1];
	ld.param.u64 	%rd26, [cast_bf16_f64_param_2];
	ld.param.u64 	%rd27, [cast_bf16_f64_param_3];
	ld.param.u64 	%rd28, [cast_bf16_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB4_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB4_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB4_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB4_5;

$L__BB4_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB4_2;

$L__BB4_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB4_15;
	bra.uni 	$L__BB4_6;

$L__BB4_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB4_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB4_17:
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs3, [%rd56];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	cvt.f64.f32 	%fd3, %f3;
	shl.b64 	%rd57, %rd60, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB4_17;
	bra.uni 	$L__BB4_18;

$L__BB4_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB4_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB4_14;

$L__BB4_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB4_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB4_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB4_12;

$L__BB4_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB4_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB4_9;

	mul.wide.u32 	%rd49, %r38, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvt.f64.f32 	%fd1, %f1;
	shl.b64 	%rd51, %rd60, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB4_8;
	bra.uni 	$L__BB4_18;

$L__BB4_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	cvt.f64.f32 	%fd2, %f2;
	shl.b64 	%rd53, %rd60, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB4_14;

$L__BB4_18:
	ret;

}
	// .globl	cast_f16_bf16
.visible .entry cast_f16_bf16(
	.param .u64 cast_f16_bf16_param_0,
	.param .u64 cast_f16_bf16_param_1,
	.param .u64 cast_f16_bf16_param_2,
	.param .u64 cast_f16_bf16_param_3,
	.param .u64 cast_f16_bf16_param_4
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<22>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f16_bf16_param_0];
	ld.param.u64 	%rd25, [cast_f16_bf16_param_1];
	ld.param.u64 	%rd26, [cast_f16_bf16_param_2];
	ld.param.u64 	%rd27, [cast_f16_bf16_param_3];
	ld.param.u64 	%rd28, [cast_f16_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p37, %p2;
	@%p5 bra 	$L__BB5_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r49, 0;

$L__BB5_2:
	not.b32 	%r23, %r49;
	cvt.u64.u32 	%rd30, %r23;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB5_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p37, 0;
	@%p8 bra 	$L__BB5_5;

$L__BB5_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd37, %r49;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p37, %p2;
	@%p10 bra 	$L__BB5_2;

$L__BB5_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r50, %r24, %r3, %r25;
	cvt.u64.u32 	%rd60, %r50;
	@%p37 bra 	$L__BB5_21;
	bra.uni 	$L__BB5_6;

$L__BB5_21:
	setp.ge.u64 	%p29, %rd60, %rd24;
	@%p29 bra 	$L__BB5_27;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r44;

$L__BB5_23:
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs16, [%rd56];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs16;}

	// end inline asm
	mov.b32 	%r45, %f3;
	and.b32  	%r46, %r45, 2147483647;
	setp.gt.u32 	%p30, %r46, 2139095040;
	shl.b32 	%r47, %r45, 16;
	shr.u32 	%r48, %r45, 16;
	cvt.u16.u32 	%rs17, %r48;
	selp.b32 	%r20, 0, %r47, %p30;
	selp.b16 	%rs21, 32767, %rs17, %p30;
	setp.gt.u32 	%p31, %r20, -2147483648;
	@%p31 bra 	$L__BB5_25;

	setp.ne.s32 	%p32, %r20, -2147483648;
	and.b16  	%rs18, %rs21, 1;
	setp.eq.b16 	%p33, %rs18, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB5_26;

$L__BB5_25:
	add.s16 	%rs21, %rs21, 1;

$L__BB5_26:
	add.s64 	%rd58, %rd1, %rd55;
	st.global.u16 	[%rd58], %rs21;
	add.s32 	%r50, %r50, %r18;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p36, %rd60, %rd24;
	@%p36 bra 	$L__BB5_23;
	bra.uni 	$L__BB5_27;

$L__BB5_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB5_27;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r26;
	@%p3 bra 	$L__BB5_17;

$L__BB5_8:
	mov.u32 	%r51, 0;
	mov.u32 	%r52, %r50;
	mov.u32 	%r53, %r51;

$L__BB5_9:
	not.b32 	%r29, %r51;
	cvt.u64.u32 	%rd38, %r29;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r52;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB5_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB5_12;

$L__BB5_11:
	cvt.u32.u64 	%r30, %rd12;
	cvt.u32.u64 	%r31, %rd10;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd61, %r32;
	cvt.u64.u32 	%rd62, %r34;

$L__BB5_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r35, %rd47;
	add.s32 	%r53, %r53, %r35;
	cvt.u32.u64 	%r52, %rd61;
	add.s32 	%r51, %r51, 1;
	cvt.u64.u32 	%rd48, %r51;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB5_9;

	mul.wide.u32 	%rd49, %r53, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs10, [%rd50];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs10;}

	// end inline asm
	mov.b32 	%r36, %f1;
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p15, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs11, %r39;
	selp.b32 	%r13, 0, %r38, %p15;
	selp.b16 	%rs19, 32767, %rs11, %p15;
	setp.gt.u32 	%p16, %r13, -2147483648;
	@%p16 bra 	$L__BB5_15;

	setp.ne.s32 	%p17, %r13, -2147483648;
	and.b16  	%rs12, %rs19, 1;
	setp.eq.b16 	%p18, %rs12, 1;
	not.pred 	%p19, %p18;
	or.pred  	%p20, %p17, %p19;
	@%p20 bra 	$L__BB5_16;

$L__BB5_15:
	add.s16 	%rs19, %rs19, 1;

$L__BB5_16:
	shl.b64 	%rd51, %rd60, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs19;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p21, %rd60, %rd24;
	@%p21 bra 	$L__BB5_8;
	bra.uni 	$L__BB5_27;

$L__BB5_17:
	ld.global.u16 	%rs13, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs13;}

	// end inline asm
	mov.b32 	%r40, %f2;
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs14, %r43;
	selp.b32 	%r16, 0, %r42, %p22;
	selp.b16 	%rs20, 32767, %rs14, %p22;
	setp.gt.u32 	%p23, %r16, -2147483648;
	@%p23 bra 	$L__BB5_19;

	setp.ne.s32 	%p24, %r16, -2147483648;
	and.b16  	%rs15, %rs20, 1;
	setp.eq.b16 	%p25, %rs15, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB5_20;

$L__BB5_19:
	add.s16 	%rs20, %rs20, 1;

$L__BB5_20:
	shl.b64 	%rd53, %rd60, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs20;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p28, %rd60, %rd24;
	@%p28 bra 	$L__BB5_17;

$L__BB5_27:
	ret;

}
	// .globl	cast_f64_bf16
.visible .entry cast_f64_bf16(
	.param .u64 cast_f64_bf16_param_0,
	.param .u64 cast_f64_bf16_param_1,
	.param .u64 cast_f64_bf16_param_2,
	.param .u64 cast_f64_bf16_param_3,
	.param .u64 cast_f64_bf16_param_4
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<19>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<56>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f64_bf16_param_0];
	ld.param.u64 	%rd25, [cast_f64_bf16_param_1];
	ld.param.u64 	%rd26, [cast_f64_bf16_param_2];
	ld.param.u64 	%rd27, [cast_f64_bf16_param_3];
	ld.param.u64 	%rd28, [cast_f64_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p37, %p2;
	@%p5 bra 	$L__BB6_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r49, 0;

$L__BB6_2:
	not.b32 	%r23, %r49;
	cvt.u64.u32 	%rd30, %r23;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB6_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p37, 0;
	@%p8 bra 	$L__BB6_5;

$L__BB6_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd37, %r49;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p37, %p2;
	@%p10 bra 	$L__BB6_2;

$L__BB6_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r50, %r24, %r3, %r25;
	cvt.u64.u32 	%rd60, %r50;
	@%p37 bra 	$L__BB6_21;
	bra.uni 	$L__BB6_6;

$L__BB6_21:
	setp.ge.u64 	%p29, %rd60, %rd24;
	@%p29 bra 	$L__BB6_27;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r44;

$L__BB6_23:
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd3, [%rd56];
	cvt.rn.f32.f64 	%f3, %fd3;
	mov.b32 	%r45, %f3;
	and.b32  	%r46, %r45, 2147483647;
	setp.gt.u32 	%p30, %r46, 2139095040;
	shl.b32 	%r47, %r45, 16;
	shr.u32 	%r48, %r45, 16;
	cvt.u16.u32 	%rs14, %r48;
	selp.b32 	%r20, 0, %r47, %p30;
	selp.b16 	%rs18, 32767, %rs14, %p30;
	setp.gt.u32 	%p31, %r20, -2147483648;
	@%p31 bra 	$L__BB6_25;

	setp.ne.s32 	%p32, %r20, -2147483648;
	and.b16  	%rs15, %rs18, 1;
	setp.eq.b16 	%p33, %rs15, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB6_26;

$L__BB6_25:
	add.s16 	%rs18, %rs18, 1;

$L__BB6_26:
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs18;
	add.s32 	%r50, %r50, %r18;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p36, %rd60, %rd24;
	@%p36 bra 	$L__BB6_23;
	bra.uni 	$L__BB6_27;

$L__BB6_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB6_27;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r26;
	@%p3 bra 	$L__BB6_17;

$L__BB6_8:
	mov.u32 	%r51, 0;
	mov.u32 	%r52, %r50;
	mov.u32 	%r53, %r51;

$L__BB6_9:
	not.b32 	%r29, %r51;
	cvt.u64.u32 	%rd38, %r29;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r52;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB6_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB6_12;

$L__BB6_11:
	cvt.u32.u64 	%r30, %rd12;
	cvt.u32.u64 	%r31, %rd10;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd61, %r32;
	cvt.u64.u32 	%rd62, %r34;

$L__BB6_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r35, %rd47;
	add.s32 	%r53, %r53, %r35;
	cvt.u32.u64 	%r52, %rd61;
	add.s32 	%r51, %r51, 1;
	cvt.u64.u32 	%rd48, %r51;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB6_9;

	mul.wide.u32 	%rd49, %r53, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rn.f32.f64 	%f1, %fd1;
	mov.b32 	%r36, %f1;
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p15, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs10, %r39;
	selp.b32 	%r13, 0, %r38, %p15;
	selp.b16 	%rs16, 32767, %rs10, %p15;
	setp.gt.u32 	%p16, %r13, -2147483648;
	@%p16 bra 	$L__BB6_15;

	setp.ne.s32 	%p17, %r13, -2147483648;
	and.b16  	%rs11, %rs16, 1;
	setp.eq.b16 	%p18, %rs11, 1;
	not.pred 	%p19, %p18;
	or.pred  	%p20, %p17, %p19;
	@%p20 bra 	$L__BB6_16;

$L__BB6_15:
	add.s16 	%rs16, %rs16, 1;

$L__BB6_16:
	shl.b64 	%rd51, %rd60, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs16;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p21, %rd60, %rd24;
	@%p21 bra 	$L__BB6_8;
	bra.uni 	$L__BB6_27;

$L__BB6_17:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rn.f32.f64 	%f2, %fd2;
	mov.b32 	%r40, %f2;
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs12, %r43;
	selp.b32 	%r16, 0, %r42, %p22;
	selp.b16 	%rs17, 32767, %rs12, %p22;
	setp.gt.u32 	%p23, %r16, -2147483648;
	@%p23 bra 	$L__BB6_19;

	setp.ne.s32 	%p24, %r16, -2147483648;
	and.b16  	%rs13, %rs17, 1;
	setp.eq.b16 	%p25, %rs13, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB6_20;

$L__BB6_19:
	add.s16 	%rs17, %rs17, 1;

$L__BB6_20:
	shl.b64 	%rd53, %rd60, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs17;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p28, %rd60, %rd24;
	@%p28 bra 	$L__BB6_17;

$L__BB6_27:
	ret;

}
	// .globl	cast_u8_bf16
.visible .entry cast_u8_bf16(
	.param .u64 cast_u8_bf16_param_0,
	.param .u64 cast_u8_bf16_param_1,
	.param .u64 cast_u8_bf16_param_2,
	.param .u64 cast_u8_bf16_param_3,
	.param .u64 cast_u8_bf16_param_4
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<22>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u8_bf16_param_0];
	ld.param.u64 	%rd25, [cast_u8_bf16_param_1];
	ld.param.u64 	%rd26, [cast_u8_bf16_param_2];
	ld.param.u64 	%rd27, [cast_u8_bf16_param_3];
	ld.param.u64 	%rd28, [cast_u8_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p37, %p2;
	@%p5 bra 	$L__BB7_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r49, 0;

$L__BB7_2:
	not.b32 	%r23, %r49;
	cvt.u64.u32 	%rd30, %r23;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB7_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p37, 0;
	@%p8 bra 	$L__BB7_5;

$L__BB7_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd37, %r49;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p37, %p2;
	@%p10 bra 	$L__BB7_2;

$L__BB7_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r50, %r24, %r3, %r25;
	cvt.u64.u32 	%rd59, %r50;
	@%p37 bra 	$L__BB7_21;
	bra.uni 	$L__BB7_6;

$L__BB7_21:
	setp.ge.u64 	%p29, %rd59, %rd24;
	@%p29 bra 	$L__BB7_27;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r44;

$L__BB7_23:
	add.s64 	%rd55, %rd2, %rd59;
	ld.global.u8 	%rs16, [%rd55];
	cvt.rn.f32.u16 	%f3, %rs16;
	mov.b32 	%r45, %f3;
	and.b32  	%r46, %r45, 2147483647;
	setp.gt.u32 	%p30, %r46, 2139095040;
	shl.b32 	%r47, %r45, 16;
	shr.u32 	%r48, %r45, 16;
	cvt.u16.u32 	%rs17, %r48;
	selp.b32 	%r20, 0, %r47, %p30;
	selp.b16 	%rs21, 32767, %rs17, %p30;
	setp.gt.u32 	%p31, %r20, -2147483648;
	@%p31 bra 	$L__BB7_25;

	setp.ne.s32 	%p32, %r20, -2147483648;
	and.b16  	%rs18, %rs21, 1;
	setp.eq.b16 	%p33, %rs18, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB7_26;

$L__BB7_25:
	add.s16 	%rs21, %rs21, 1;

$L__BB7_26:
	shl.b64 	%rd56, %rd59, 1;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u16 	[%rd57], %rs21;
	add.s32 	%r50, %r50, %r18;
	cvt.u64.u32 	%rd59, %r50;
	setp.lt.u64 	%p36, %rd59, %rd24;
	@%p36 bra 	$L__BB7_23;
	bra.uni 	$L__BB7_27;

$L__BB7_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB7_27;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r26;
	@%p3 bra 	$L__BB7_17;

$L__BB7_8:
	mov.u32 	%r51, 0;
	mov.u32 	%r52, %r50;
	mov.u32 	%r53, %r51;

$L__BB7_9:
	not.b32 	%r29, %r51;
	cvt.u64.u32 	%rd38, %r29;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r52;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB7_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB7_12;

$L__BB7_11:
	cvt.u32.u64 	%r30, %rd12;
	cvt.u32.u64 	%r31, %rd10;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd60, %r32;
	cvt.u64.u32 	%rd61, %r34;

$L__BB7_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r35, %rd47;
	add.s32 	%r53, %r53, %r35;
	cvt.u32.u64 	%r52, %rd60;
	add.s32 	%r51, %r51, 1;
	cvt.u64.u32 	%rd48, %r51;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB7_9;

	cvt.u64.u32 	%rd49, %r53;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rs10, [%rd50];
	cvt.rn.f32.u16 	%f1, %rs10;
	mov.b32 	%r36, %f1;
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p15, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs11, %r39;
	selp.b32 	%r13, 0, %r38, %p15;
	selp.b16 	%rs19, 32767, %rs11, %p15;
	setp.gt.u32 	%p16, %r13, -2147483648;
	@%p16 bra 	$L__BB7_15;

	setp.ne.s32 	%p17, %r13, -2147483648;
	and.b16  	%rs12, %rs19, 1;
	setp.eq.b16 	%p18, %rs12, 1;
	not.pred 	%p19, %p18;
	or.pred  	%p20, %p17, %p19;
	@%p20 bra 	$L__BB7_16;

$L__BB7_15:
	add.s16 	%rs19, %rs19, 1;

$L__BB7_16:
	shl.b64 	%rd51, %rd59, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs19;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd59, %r50;
	setp.lt.u64 	%p21, %rd59, %rd24;
	@%p21 bra 	$L__BB7_8;
	bra.uni 	$L__BB7_27;

$L__BB7_17:
	ld.global.u8 	%rs13, [%rd2];
	cvt.rn.f32.u16 	%f2, %rs13;
	mov.b32 	%r40, %f2;
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs14, %r43;
	selp.b32 	%r16, 0, %r42, %p22;
	selp.b16 	%rs20, 32767, %rs14, %p22;
	setp.gt.u32 	%p23, %r16, -2147483648;
	@%p23 bra 	$L__BB7_19;

	setp.ne.s32 	%p24, %r16, -2147483648;
	and.b16  	%rs15, %rs20, 1;
	setp.eq.b16 	%p25, %rs15, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB7_20;

$L__BB7_19:
	add.s16 	%rs20, %rs20, 1;

$L__BB7_20:
	shl.b64 	%rd53, %rd59, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs20;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd59, %r50;
	setp.lt.u64 	%p28, %rd59, %rd24;
	@%p28 bra 	$L__BB7_17;

$L__BB7_27:
	ret;

}
	// .globl	cast_f16_f16
.visible .entry cast_f16_f16(
	.param .u64 cast_f16_f16_param_0,
	.param .u64 cast_f16_f16_param_1,
	.param .u64 cast_f16_f16_param_2,
	.param .u64 cast_f16_f16_param_3,
	.param .u64 cast_f16_f16_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_f16_f16_param_0];
	ld.param.u64 	%rd25, [cast_f16_f16_param_1];
	ld.param.u64 	%rd26, [cast_f16_f16_param_2];
	ld.param.u64 	%rd27, [cast_f16_f16_param_3];
	ld.param.u64 	%rd28, [cast_f16_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB8_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB8_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB8_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB8_5;

$L__BB8_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB8_2;

$L__BB8_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB8_15;
	bra.uni 	$L__BB8_6;

$L__BB8_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB8_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB8_17:
	shl.b64 	%rd55, %rd59, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs3, [%rd56];
	add.s64 	%rd57, %rd1, %rd55;
	st.global.u16 	[%rd57], %rs3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB8_17;
	bra.uni 	$L__BB8_18;

$L__BB8_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB8_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB8_14;

$L__BB8_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB8_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB8_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB8_12;

$L__BB8_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB8_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB8_9;

	mul.wide.u32 	%rd49, %r38, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	shl.b64 	%rd51, %rd59, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB8_8;
	bra.uni 	$L__BB8_18;

$L__BB8_14:
	ld.global.u16 	%rs2, [%rd2];
	shl.b64 	%rd53, %rd59, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB8_14;

$L__BB8_18:
	ret;

}
	// .globl	cast_f16_u8
.visible .entry cast_f16_u8(
	.param .u64 cast_f16_u8_param_0,
	.param .u64 cast_f16_u8_param_1,
	.param .u64 cast_f16_u8_param_2,
	.param .u64 cast_f16_u8_param_3,
	.param .u64 cast_f16_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd24, [cast_f16_u8_param_0];
	ld.param.u64 	%rd25, [cast_f16_u8_param_1];
	ld.param.u64 	%rd26, [cast_f16_u8_param_2];
	ld.param.u64 	%rd27, [cast_f16_u8_param_3];
	ld.param.u64 	%rd28, [cast_f16_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB9_5;

	mov.u64 	%rd56, 1;
	mov.u32 	%r37, 0;

$L__BB9_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB9_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd56, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB9_5;

$L__BB9_4:
	mul.lo.s64 	%rd56, %rd6, %rd56;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB9_2;

$L__BB9_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r38;
	@%p19 bra 	$L__BB9_15;
	bra.uni 	$L__BB9_6;

$L__BB9_15:
	setp.ge.u64 	%p17, %rd57, %rd24;
	@%p17 bra 	$L__BB9_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB9_17:
	shl.b64 	%rd53, %rd57, 1;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u16 	%rs3, [%rd54];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p18, %rd57, %rd24;
	@%p18 bra 	$L__BB9_17;
	bra.uni 	$L__BB9_18;

$L__BB9_6:
	setp.ge.u64 	%p11, %rd57, %rd24;
	@%p11 bra 	$L__BB9_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB9_14;

$L__BB9_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB9_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB9_11;

	div.u64 	%rd58, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd58, %rd12;
	sub.s64 	%rd59, %rd10, %rd43;
	bra.uni 	$L__BB9_12;

$L__BB9_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd58, %r29;
	cvt.u64.u32 	%rd59, %r31;

$L__BB9_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd59;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB9_9;

	mul.wide.u32 	%rd49, %r41, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	cvt.rzi.u32.f32 	%r33, %f1;
	add.s64 	%rd51, %rd1, %rd57;
	st.global.u8 	[%rd51], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p15, %rd57, %rd24;
	@%p15 bra 	$L__BB9_8;
	bra.uni 	$L__BB9_18;

$L__BB9_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	cvt.rzi.u32.f32 	%r34, %f2;
	add.s64 	%rd52, %rd1, %rd57;
	st.global.u8 	[%rd52], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p16, %rd57, %rd24;
	@%p16 bra 	$L__BB9_14;

$L__BB9_18:
	ret;

}
	// .globl	cast_f16_u32
.visible .entry cast_f16_u32(
	.param .u64 cast_f16_u32_param_0,
	.param .u64 cast_f16_u32_param_1,
	.param .u64 cast_f16_u32_param_2,
	.param .u64 cast_f16_u32_param_3,
	.param .u64 cast_f16_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f16_u32_param_0];
	ld.param.u64 	%rd25, [cast_f16_u32_param_1];
	ld.param.u64 	%rd26, [cast_f16_u32_param_2];
	ld.param.u64 	%rd27, [cast_f16_u32_param_3];
	ld.param.u64 	%rd28, [cast_f16_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB10_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB10_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB10_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB10_5;

$L__BB10_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB10_2;

$L__BB10_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p19 bra 	$L__BB10_15;
	bra.uni 	$L__BB10_6;

$L__BB10_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB10_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB10_17:
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs3, [%rd56];
	// begin inline asm
	cvt.rzi.u32.f16 %r36, %rs3;
	// end inline asm
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u32 	[%rd58], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB10_17;
	bra.uni 	$L__BB10_18;

$L__BB10_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB10_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB10_14;

$L__BB10_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB10_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB10_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB10_12;

$L__BB10_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB10_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd61;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB10_9;

	mul.wide.u32 	%rd49, %r41, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	cvt.rzi.u32.f16 %r33, %rs1;
	// end inline asm
	shl.b64 	%rd51, %rd60, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB10_8;
	bra.uni 	$L__BB10_18;

$L__BB10_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	cvt.rzi.u32.f16 %r34, %rs2;
	// end inline asm
	shl.b64 	%rd53, %rd60, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB10_14;

$L__BB10_18:
	ret;

}
	// .globl	cast_f16_f32
.visible .entry cast_f16_f32(
	.param .u64 cast_f16_f32_param_0,
	.param .u64 cast_f16_f32_param_1,
	.param .u64 cast_f16_f32_param_2,
	.param .u64 cast_f16_f32_param_3,
	.param .u64 cast_f16_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f16_f32_param_0];
	ld.param.u64 	%rd25, [cast_f16_f32_param_1];
	ld.param.u64 	%rd26, [cast_f16_f32_param_2];
	ld.param.u64 	%rd27, [cast_f16_f32_param_3];
	ld.param.u64 	%rd28, [cast_f16_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB11_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB11_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB11_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB11_5;

$L__BB11_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB11_2;

$L__BB11_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB11_15;
	bra.uni 	$L__BB11_6;

$L__BB11_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB11_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB11_17:
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs3, [%rd56];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f32 	[%rd58], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB11_17;
	bra.uni 	$L__BB11_18;

$L__BB11_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB11_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB11_14;

$L__BB11_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB11_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB11_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB11_12;

$L__BB11_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB11_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB11_9;

	mul.wide.u32 	%rd49, %r38, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	shl.b64 	%rd51, %rd60, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB11_8;
	bra.uni 	$L__BB11_18;

$L__BB11_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	shl.b64 	%rd53, %rd60, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB11_14;

$L__BB11_18:
	ret;

}
	// .globl	cast_f16_f64
.visible .entry cast_f16_f64(
	.param .u64 cast_f16_f64_param_0,
	.param .u64 cast_f16_f64_param_1,
	.param .u64 cast_f16_f64_param_2,
	.param .u64 cast_f16_f64_param_3,
	.param .u64 cast_f16_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f16_f64_param_0];
	ld.param.u64 	%rd25, [cast_f16_f64_param_1];
	ld.param.u64 	%rd26, [cast_f16_f64_param_2];
	ld.param.u64 	%rd27, [cast_f16_f64_param_3];
	ld.param.u64 	%rd28, [cast_f16_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB12_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB12_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB12_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB12_5;

$L__BB12_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB12_2;

$L__BB12_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB12_15;
	bra.uni 	$L__BB12_6;

$L__BB12_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB12_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB12_17:
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs3, [%rd56];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// end inline asm
	cvt.f64.f32 	%fd3, %f3;
	shl.b64 	%rd57, %rd60, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB12_17;
	bra.uni 	$L__BB12_18;

$L__BB12_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB12_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB12_14;

$L__BB12_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB12_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB12_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB12_12;

$L__BB12_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB12_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB12_9;

	mul.wide.u32 	%rd49, %r38, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	cvt.f64.f32 	%fd1, %f1;
	shl.b64 	%rd51, %rd60, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB12_8;
	bra.uni 	$L__BB12_18;

$L__BB12_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// end inline asm
	cvt.f64.f32 	%fd2, %f2;
	shl.b64 	%rd53, %rd60, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB12_14;

$L__BB12_18:
	ret;

}
	// .globl	cast_u8_f16
.visible .entry cast_u8_f16(
	.param .u64 cast_u8_f16_param_0,
	.param .u64 cast_u8_f16_param_1,
	.param .u64 cast_u8_f16_param_2,
	.param .u64 cast_u8_f16_param_3,
	.param .u64 cast_u8_f16_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u8_f16_param_0];
	ld.param.u64 	%rd25, [cast_u8_f16_param_1];
	ld.param.u64 	%rd26, [cast_u8_f16_param_2];
	ld.param.u64 	%rd27, [cast_u8_f16_param_3];
	ld.param.u64 	%rd28, [cast_u8_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB13_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r37, 0;

$L__BB13_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB13_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB13_5;

$L__BB13_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB13_2;

$L__BB13_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r38;
	@%p19 bra 	$L__BB13_15;
	bra.uni 	$L__BB13_6;

$L__BB13_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB13_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB13_17:
	add.s64 	%rd55, %rd2, %rd59;
	ld.global.u8 	%r36, [%rd55];
	// begin inline asm
	cvt.rn.f16.s32 %rs3, %r36;
	// end inline asm
	shl.b64 	%rd56, %rd59, 1;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u16 	[%rd57], %rs3;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB13_17;
	bra.uni 	$L__BB13_18;

$L__BB13_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB13_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB13_14;

$L__BB13_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB13_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB13_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB13_12;

$L__BB13_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB13_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd60;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB13_9;

	cvt.u64.u32 	%rd49, %r41;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%r33, [%rd50];
	// begin inline asm
	cvt.rn.f16.s32 %rs1, %r33;
	// end inline asm
	shl.b64 	%rd51, %rd59, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB13_8;
	bra.uni 	$L__BB13_18;

$L__BB13_14:
	ld.global.u8 	%r34, [%rd2];
	// begin inline asm
	cvt.rn.f16.s32 %rs2, %r34;
	// end inline asm
	shl.b64 	%rd53, %rd59, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs2;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB13_14;

$L__BB13_18:
	ret;

}
	// .globl	cast_u32_f16
.visible .entry cast_u32_f16(
	.param .u64 cast_u32_f16_param_0,
	.param .u64 cast_u32_f16_param_1,
	.param .u64 cast_u32_f16_param_2,
	.param .u64 cast_u32_f16_param_3,
	.param .u64 cast_u32_f16_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_u32_f16_param_0];
	ld.param.u64 	%rd25, [cast_u32_f16_param_1];
	ld.param.u64 	%rd26, [cast_u32_f16_param_2];
	ld.param.u64 	%rd27, [cast_u32_f16_param_3];
	ld.param.u64 	%rd28, [cast_u32_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB14_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB14_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB14_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB14_5;

$L__BB14_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB14_2;

$L__BB14_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p19 bra 	$L__BB14_15;
	bra.uni 	$L__BB14_6;

$L__BB14_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB14_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB14_17:
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r36, [%rd56];
	// begin inline asm
	cvt.rn.f16.u32 %rs3, %r36;
	// end inline asm
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs3;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB14_17;
	bra.uni 	$L__BB14_18;

$L__BB14_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB14_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB14_14;

$L__BB14_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB14_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB14_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB14_12;

$L__BB14_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB14_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd61;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB14_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r33, [%rd50];
	// begin inline asm
	cvt.rn.f16.u32 %rs1, %r33;
	// end inline asm
	shl.b64 	%rd51, %rd60, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB14_8;
	bra.uni 	$L__BB14_18;

$L__BB14_14:
	ld.global.u32 	%r34, [%rd2];
	// begin inline asm
	cvt.rn.f16.u32 %rs2, %r34;
	// end inline asm
	shl.b64 	%rd53, %rd60, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs2;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB14_14;

$L__BB14_18:
	ret;

}
	// .globl	cast_f32_f16
.visible .entry cast_f32_f16(
	.param .u64 cast_f32_f16_param_0,
	.param .u64 cast_f32_f16_param_1,
	.param .u64 cast_f32_f16_param_2,
	.param .u64 cast_f32_f16_param_3,
	.param .u64 cast_f32_f16_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f32_f16_param_0];
	ld.param.u64 	%rd25, [cast_f32_f16_param_1];
	ld.param.u64 	%rd26, [cast_f32_f16_param_2];
	ld.param.u64 	%rd27, [cast_f32_f16_param_3];
	ld.param.u64 	%rd28, [cast_f32_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB15_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB15_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB15_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB15_5;

$L__BB15_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB15_2;

$L__BB15_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB15_15;
	bra.uni 	$L__BB15_6;

$L__BB15_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB15_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB15_17:
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f3, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f3;}

	// end inline asm
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB15_17;
	bra.uni 	$L__BB15_18;

$L__BB15_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB15_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB15_14;

$L__BB15_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB15_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB15_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB15_12;

$L__BB15_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB15_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB15_9;

	mul.wide.u32 	%rd49, %r38, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f1;}

	// end inline asm
	shl.b64 	%rd51, %rd60, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB15_8;
	bra.uni 	$L__BB15_18;

$L__BB15_14:
	ld.global.f32 	%f2, [%rd2];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	shl.b64 	%rd53, %rd60, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB15_14;

$L__BB15_18:
	ret;

}
	// .globl	cast_f64_f16
.visible .entry cast_f64_f16(
	.param .u64 cast_f64_f16_param_0,
	.param .u64 cast_f64_f16_param_1,
	.param .u64 cast_f64_f16_param_2,
	.param .u64 cast_f64_f16_param_3,
	.param .u64 cast_f64_f16_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f64_f16_param_0];
	ld.param.u64 	%rd25, [cast_f64_f16_param_1];
	ld.param.u64 	%rd26, [cast_f64_f16_param_2];
	ld.param.u64 	%rd27, [cast_f64_f16_param_3];
	ld.param.u64 	%rd28, [cast_f64_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB16_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB16_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB16_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB16_5;

$L__BB16_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB16_2;

$L__BB16_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB16_15;
	bra.uni 	$L__BB16_6;

$L__BB16_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB16_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB16_17:
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd3, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f64 %rs3, %fd3;}

	// end inline asm
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB16_17;
	bra.uni 	$L__BB16_18;

$L__BB16_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB16_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB16_14;

$L__BB16_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB16_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB16_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB16_12;

$L__BB16_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB16_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB16_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	// begin inline asm
	{  cvt.rn.f16.f64 %rs1, %fd1;}

	// end inline asm
	shl.b64 	%rd51, %rd60, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB16_8;
	bra.uni 	$L__BB16_18;

$L__BB16_14:
	ld.global.f64 	%fd2, [%rd2];
	// begin inline asm
	{  cvt.rn.f16.f64 %rs2, %fd2;}

	// end inline asm
	shl.b64 	%rd53, %rd60, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB16_14;

$L__BB16_18:
	ret;

}
	// .globl	cast_u32_u32
.visible .entry cast_u32_u32(
	.param .u64 cast_u32_u32_param_0,
	.param .u64 cast_u32_u32_param_1,
	.param .u64 cast_u32_u32_param_2,
	.param .u64 cast_u32_u32_param_3,
	.param .u64 cast_u32_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u32_u32_param_0];
	ld.param.u64 	%rd25, [cast_u32_u32_param_1];
	ld.param.u64 	%rd26, [cast_u32_u32_param_2];
	ld.param.u64 	%rd27, [cast_u32_u32_param_3];
	ld.param.u64 	%rd28, [cast_u32_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB17_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r37, 0;

$L__BB17_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB17_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB17_5;

$L__BB17_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB17_2;

$L__BB17_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r38;
	@%p19 bra 	$L__BB17_15;
	bra.uni 	$L__BB17_6;

$L__BB17_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB17_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB17_17:
	shl.b64 	%rd55, %rd59, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r36, [%rd56];
	add.s64 	%rd57, %rd1, %rd55;
	st.global.u32 	[%rd57], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB17_17;
	bra.uni 	$L__BB17_18;

$L__BB17_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB17_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB17_14;

$L__BB17_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB17_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB17_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB17_12;

$L__BB17_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB17_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd60;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB17_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r33, [%rd50];
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB17_8;
	bra.uni 	$L__BB17_18;

$L__BB17_14:
	ld.global.u32 	%r34, [%rd2];
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB17_14;

$L__BB17_18:
	ret;

}
	// .globl	cast_u32_u8
.visible .entry cast_u32_u8(
	.param .u64 cast_u32_u8_param_0,
	.param .u64 cast_u32_u8_param_1,
	.param .u64 cast_u32_u8_param_2,
	.param .u64 cast_u32_u8_param_3,
	.param .u64 cast_u32_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd24, [cast_u32_u8_param_0];
	ld.param.u64 	%rd25, [cast_u32_u8_param_1];
	ld.param.u64 	%rd26, [cast_u32_u8_param_2];
	ld.param.u64 	%rd27, [cast_u32_u8_param_3];
	ld.param.u64 	%rd28, [cast_u32_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB18_5;

	mov.u64 	%rd56, 1;
	mov.u32 	%r37, 0;

$L__BB18_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB18_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd56, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB18_5;

$L__BB18_4:
	mul.lo.s64 	%rd56, %rd6, %rd56;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB18_2;

$L__BB18_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r38;
	@%p19 bra 	$L__BB18_15;
	bra.uni 	$L__BB18_6;

$L__BB18_15:
	setp.ge.u64 	%p17, %rd57, %rd24;
	@%p17 bra 	$L__BB18_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB18_17:
	shl.b64 	%rd53, %rd57, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u32 	%r36, [%rd54];
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p18, %rd57, %rd24;
	@%p18 bra 	$L__BB18_17;
	bra.uni 	$L__BB18_18;

$L__BB18_6:
	setp.ge.u64 	%p11, %rd57, %rd24;
	@%p11 bra 	$L__BB18_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB18_14;

$L__BB18_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB18_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB18_11;

	div.u64 	%rd58, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd58, %rd12;
	sub.s64 	%rd59, %rd10, %rd43;
	bra.uni 	$L__BB18_12;

$L__BB18_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd58, %r29;
	cvt.u64.u32 	%rd59, %r31;

$L__BB18_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd59;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB18_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r33, [%rd50];
	add.s64 	%rd51, %rd1, %rd57;
	st.global.u8 	[%rd51], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p15, %rd57, %rd24;
	@%p15 bra 	$L__BB18_8;
	bra.uni 	$L__BB18_18;

$L__BB18_14:
	ld.global.u32 	%r34, [%rd2];
	add.s64 	%rd52, %rd1, %rd57;
	st.global.u8 	[%rd52], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p16, %rd57, %rd24;
	@%p16 bra 	$L__BB18_14;

$L__BB18_18:
	ret;

}
	// .globl	cast_u32_i64
.visible .entry cast_u32_i64(
	.param .u64 cast_u32_i64_param_0,
	.param .u64 cast_u32_i64_param_1,
	.param .u64 cast_u32_i64_param_2,
	.param .u64 cast_u32_i64_param_3,
	.param .u64 cast_u32_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [cast_u32_i64_param_0];
	ld.param.u64 	%rd25, [cast_u32_i64_param_1];
	ld.param.u64 	%rd26, [cast_u32_i64_param_2];
	ld.param.u64 	%rd27, [cast_u32_i64_param_3];
	ld.param.u64 	%rd28, [cast_u32_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB19_5;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB19_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB19_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd62, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB19_5;

$L__BB19_4:
	mul.lo.s64 	%rd62, %rd6, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB19_2;

$L__BB19_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p19 bra 	$L__BB19_15;
	bra.uni 	$L__BB19_6;

$L__BB19_15:
	setp.ge.u64 	%p17, %rd63, %rd24;
	@%p17 bra 	$L__BB19_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB19_17:
	shl.b64 	%rd57, %rd63, 2;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u32 	%rd59, [%rd58];
	shl.b64 	%rd60, %rd63, 3;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.u64 	[%rd61], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB19_17;
	bra.uni 	$L__BB19_18;

$L__BB19_6:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB19_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB19_14;

$L__BB19_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB19_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB19_11;

	div.u64 	%rd64, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd64, %rd12;
	sub.s64 	%rd65, %rd10, %rd43;
	bra.uni 	$L__BB19_12;

$L__BB19_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd64, %r29;
	cvt.u64.u32 	%rd65, %r31;

$L__BB19_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd65;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB19_9;

	mul.wide.u32 	%rd49, %r38, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%rd51, [%rd50];
	shl.b64 	%rd52, %rd63, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd24;
	@%p15 bra 	$L__BB19_8;
	bra.uni 	$L__BB19_18;

$L__BB19_14:
	ld.global.u32 	%rd54, [%rd2];
	shl.b64 	%rd55, %rd63, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB19_14;

$L__BB19_18:
	ret;

}
	// .globl	cast_u32_f32
.visible .entry cast_u32_f32(
	.param .u64 cast_u32_f32_param_0,
	.param .u64 cast_u32_f32_param_1,
	.param .u64 cast_u32_f32_param_2,
	.param .u64 cast_u32_f32_param_3,
	.param .u64 cast_u32_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u32_f32_param_0];
	ld.param.u64 	%rd25, [cast_u32_f32_param_1];
	ld.param.u64 	%rd26, [cast_u32_f32_param_2];
	ld.param.u64 	%rd27, [cast_u32_f32_param_3];
	ld.param.u64 	%rd28, [cast_u32_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB20_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r37, 0;

$L__BB20_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB20_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB20_5;

$L__BB20_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB20_2;

$L__BB20_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r38;
	@%p19 bra 	$L__BB20_15;
	bra.uni 	$L__BB20_6;

$L__BB20_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB20_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB20_17:
	shl.b64 	%rd55, %rd59, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r36, [%rd56];
	cvt.rn.f32.u32 	%f3, %r36;
	add.s64 	%rd57, %rd1, %rd55;
	st.global.f32 	[%rd57], %f3;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB20_17;
	bra.uni 	$L__BB20_18;

$L__BB20_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB20_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB20_14;

$L__BB20_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB20_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB20_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB20_12;

$L__BB20_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB20_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd60;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB20_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r33, [%rd50];
	cvt.rn.f32.u32 	%f1, %r33;
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB20_8;
	bra.uni 	$L__BB20_18;

$L__BB20_14:
	ld.global.u32 	%r34, [%rd2];
	cvt.rn.f32.u32 	%f2, %r34;
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB20_14;

$L__BB20_18:
	ret;

}
	// .globl	cast_u32_f64
.visible .entry cast_u32_f64(
	.param .u64 cast_u32_f64_param_0,
	.param .u64 cast_u32_f64_param_1,
	.param .u64 cast_u32_f64_param_2,
	.param .u64 cast_u32_f64_param_3,
	.param .u64 cast_u32_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_u32_f64_param_0];
	ld.param.u64 	%rd25, [cast_u32_f64_param_1];
	ld.param.u64 	%rd26, [cast_u32_f64_param_2];
	ld.param.u64 	%rd27, [cast_u32_f64_param_3];
	ld.param.u64 	%rd28, [cast_u32_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB21_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB21_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB21_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB21_5;

$L__BB21_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB21_2;

$L__BB21_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p19 bra 	$L__BB21_15;
	bra.uni 	$L__BB21_6;

$L__BB21_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB21_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB21_17:
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r36, [%rd56];
	cvt.rn.f64.u32 	%fd3, %r36;
	shl.b64 	%rd57, %rd60, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd3;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB21_17;
	bra.uni 	$L__BB21_18;

$L__BB21_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB21_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB21_14;

$L__BB21_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB21_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB21_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB21_12;

$L__BB21_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB21_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd61;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB21_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r33, [%rd50];
	cvt.rn.f64.u32 	%fd1, %r33;
	shl.b64 	%rd51, %rd60, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB21_8;
	bra.uni 	$L__BB21_18;

$L__BB21_14:
	ld.global.u32 	%r34, [%rd2];
	cvt.rn.f64.u32 	%fd2, %r34;
	shl.b64 	%rd53, %rd60, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB21_14;

$L__BB21_18:
	ret;

}
	// .globl	cast_u8_u32
.visible .entry cast_u8_u32(
	.param .u64 cast_u8_u32_param_0,
	.param .u64 cast_u8_u32_param_1,
	.param .u64 cast_u8_u32_param_2,
	.param .u64 cast_u8_u32_param_3,
	.param .u64 cast_u8_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u8_u32_param_0];
	ld.param.u64 	%rd25, [cast_u8_u32_param_1];
	ld.param.u64 	%rd26, [cast_u8_u32_param_2];
	ld.param.u64 	%rd27, [cast_u8_u32_param_3];
	ld.param.u64 	%rd28, [cast_u8_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB22_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r37, 0;

$L__BB22_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB22_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB22_5;

$L__BB22_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB22_2;

$L__BB22_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r38;
	@%p19 bra 	$L__BB22_15;
	bra.uni 	$L__BB22_6;

$L__BB22_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB22_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB22_17:
	add.s64 	%rd55, %rd2, %rd59;
	ld.global.u8 	%r36, [%rd55];
	shl.b64 	%rd56, %rd59, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u32 	[%rd57], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB22_17;
	bra.uni 	$L__BB22_18;

$L__BB22_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB22_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB22_14;

$L__BB22_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB22_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB22_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB22_12;

$L__BB22_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB22_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd60;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB22_9;

	cvt.u64.u32 	%rd49, %r41;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%r33, [%rd50];
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB22_8;
	bra.uni 	$L__BB22_18;

$L__BB22_14:
	ld.global.u8 	%r34, [%rd2];
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB22_14;

$L__BB22_18:
	ret;

}
	// .globl	cast_u8_u8
.visible .entry cast_u8_u8(
	.param .u64 cast_u8_u8_param_0,
	.param .u64 cast_u8_u8_param_1,
	.param .u64 cast_u8_u8_param_2,
	.param .u64 cast_u8_u8_param_3,
	.param .u64 cast_u8_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<61>;


	ld.param.u64 	%rd24, [cast_u8_u8_param_0];
	ld.param.u64 	%rd25, [cast_u8_u8_param_1];
	ld.param.u64 	%rd26, [cast_u8_u8_param_2];
	ld.param.u64 	%rd27, [cast_u8_u8_param_3];
	ld.param.u64 	%rd28, [cast_u8_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB23_5;

	mov.u64 	%rd55, 1;
	mov.u32 	%r34, 0;

$L__BB23_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB23_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd55, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB23_5;

$L__BB23_4:
	mul.lo.s64 	%rd55, %rd6, %rd55;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB23_2;

$L__BB23_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd56, %r35;
	@%p19 bra 	$L__BB23_15;
	bra.uni 	$L__BB23_6;

$L__BB23_15:
	setp.ge.u64 	%p17, %rd56, %rd24;
	@%p17 bra 	$L__BB23_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB23_17:
	add.s64 	%rd53, %rd2, %rd56;
	ld.global.u8 	%rs3, [%rd53];
	add.s64 	%rd54, %rd1, %rd56;
	st.global.u8 	[%rd54], %rs3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd56, %r35;
	setp.lt.u64 	%p18, %rd56, %rd24;
	@%p18 bra 	$L__BB23_17;
	bra.uni 	$L__BB23_18;

$L__BB23_6:
	setp.ge.u64 	%p11, %rd56, %rd24;
	@%p11 bra 	$L__BB23_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB23_14;

$L__BB23_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB23_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB23_11;

	div.u64 	%rd57, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd57, %rd12;
	sub.s64 	%rd58, %rd10, %rd43;
	bra.uni 	$L__BB23_12;

$L__BB23_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd57, %r29;
	cvt.u64.u32 	%rd58, %r31;

$L__BB23_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd58;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd57;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB23_9;

	cvt.u64.u32 	%rd49, %r38;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rs1, [%rd50];
	add.s64 	%rd51, %rd1, %rd56;
	st.global.u8 	[%rd51], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd56, %r35;
	setp.lt.u64 	%p15, %rd56, %rd24;
	@%p15 bra 	$L__BB23_8;
	bra.uni 	$L__BB23_18;

$L__BB23_14:
	ld.global.u8 	%rs2, [%rd2];
	add.s64 	%rd52, %rd1, %rd56;
	st.global.u8 	[%rd52], %rs2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd56, %r35;
	setp.lt.u64 	%p16, %rd56, %rd24;
	@%p16 bra 	$L__BB23_14;

$L__BB23_18:
	ret;

}
	// .globl	cast_u8_i64
.visible .entry cast_u8_i64(
	.param .u64 cast_u8_i64_param_0,
	.param .u64 cast_u8_i64_param_1,
	.param .u64 cast_u8_i64_param_2,
	.param .u64 cast_u8_i64_param_3,
	.param .u64 cast_u8_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd24, [cast_u8_i64_param_0];
	ld.param.u64 	%rd25, [cast_u8_i64_param_1];
	ld.param.u64 	%rd26, [cast_u8_i64_param_2];
	ld.param.u64 	%rd27, [cast_u8_i64_param_3];
	ld.param.u64 	%rd28, [cast_u8_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB24_5;

	mov.u64 	%rd61, 1;
	mov.u32 	%r34, 0;

$L__BB24_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB24_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd61, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB24_5;

$L__BB24_4:
	mul.lo.s64 	%rd61, %rd6, %rd61;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB24_2;

$L__BB24_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd62, %r35;
	@%p19 bra 	$L__BB24_15;
	bra.uni 	$L__BB24_6;

$L__BB24_15:
	setp.ge.u64 	%p17, %rd62, %rd24;
	@%p17 bra 	$L__BB24_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB24_17:
	add.s64 	%rd57, %rd2, %rd62;
	ld.global.u8 	%rd58, [%rd57];
	shl.b64 	%rd59, %rd62, 3;
	add.s64 	%rd60, %rd1, %rd59;
	st.global.u64 	[%rd60], %rd58;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p18, %rd62, %rd24;
	@%p18 bra 	$L__BB24_17;
	bra.uni 	$L__BB24_18;

$L__BB24_6:
	setp.ge.u64 	%p11, %rd62, %rd24;
	@%p11 bra 	$L__BB24_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB24_14;

$L__BB24_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB24_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB24_11;

	div.u64 	%rd63, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd63, %rd12;
	sub.s64 	%rd64, %rd10, %rd43;
	bra.uni 	$L__BB24_12;

$L__BB24_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd63, %r29;
	cvt.u64.u32 	%rd64, %r31;

$L__BB24_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd64;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB24_9;

	cvt.u64.u32 	%rd49, %r38;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rd51, [%rd50];
	shl.b64 	%rd52, %rd62, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p15, %rd62, %rd24;
	@%p15 bra 	$L__BB24_8;
	bra.uni 	$L__BB24_18;

$L__BB24_14:
	ld.global.u8 	%rd54, [%rd2];
	shl.b64 	%rd55, %rd62, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p16, %rd62, %rd24;
	@%p16 bra 	$L__BB24_14;

$L__BB24_18:
	ret;

}
	// .globl	cast_u8_f32
.visible .entry cast_u8_f32(
	.param .u64 cast_u8_f32_param_0,
	.param .u64 cast_u8_f32_param_1,
	.param .u64 cast_u8_f32_param_2,
	.param .u64 cast_u8_f32_param_3,
	.param .u64 cast_u8_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u8_f32_param_0];
	ld.param.u64 	%rd25, [cast_u8_f32_param_1];
	ld.param.u64 	%rd26, [cast_u8_f32_param_2];
	ld.param.u64 	%rd27, [cast_u8_f32_param_3];
	ld.param.u64 	%rd28, [cast_u8_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB25_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB25_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB25_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB25_5;

$L__BB25_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB25_2;

$L__BB25_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB25_15;
	bra.uni 	$L__BB25_6;

$L__BB25_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB25_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB25_17:
	add.s64 	%rd55, %rd2, %rd59;
	ld.global.u8 	%rs3, [%rd55];
	cvt.rn.f32.u16 	%f3, %rs3;
	shl.b64 	%rd56, %rd59, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f32 	[%rd57], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB25_17;
	bra.uni 	$L__BB25_18;

$L__BB25_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB25_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB25_14;

$L__BB25_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB25_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB25_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB25_12;

$L__BB25_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB25_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB25_9;

	cvt.u64.u32 	%rd49, %r38;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rs1, [%rd50];
	cvt.rn.f32.u16 	%f1, %rs1;
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB25_8;
	bra.uni 	$L__BB25_18;

$L__BB25_14:
	ld.global.u8 	%rs2, [%rd2];
	cvt.rn.f32.u16 	%f2, %rs2;
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB25_14;

$L__BB25_18:
	ret;

}
	// .globl	cast_u8_f64
.visible .entry cast_u8_f64(
	.param .u64 cast_u8_f64_param_0,
	.param .u64 cast_u8_f64_param_1,
	.param .u64 cast_u8_f64_param_2,
	.param .u64 cast_u8_f64_param_3,
	.param .u64 cast_u8_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u8_f64_param_0];
	ld.param.u64 	%rd25, [cast_u8_f64_param_1];
	ld.param.u64 	%rd26, [cast_u8_f64_param_2];
	ld.param.u64 	%rd27, [cast_u8_f64_param_3];
	ld.param.u64 	%rd28, [cast_u8_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB26_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB26_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB26_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB26_5;

$L__BB26_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB26_2;

$L__BB26_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB26_15;
	bra.uni 	$L__BB26_6;

$L__BB26_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB26_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB26_17:
	add.s64 	%rd55, %rd2, %rd59;
	ld.global.u8 	%rs3, [%rd55];
	cvt.rn.f64.u16 	%fd3, %rs3;
	shl.b64 	%rd56, %rd59, 3;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f64 	[%rd57], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB26_17;
	bra.uni 	$L__BB26_18;

$L__BB26_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB26_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB26_14;

$L__BB26_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB26_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB26_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB26_12;

$L__BB26_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB26_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB26_9;

	cvt.u64.u32 	%rd49, %r38;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rs1, [%rd50];
	cvt.rn.f64.u16 	%fd1, %rs1;
	shl.b64 	%rd51, %rd59, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB26_8;
	bra.uni 	$L__BB26_18;

$L__BB26_14:
	ld.global.u8 	%rs2, [%rd2];
	cvt.rn.f64.u16 	%fd2, %rs2;
	shl.b64 	%rd53, %rd59, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB26_14;

$L__BB26_18:
	ret;

}
	// .globl	cast_i64_u32
.visible .entry cast_i64_u32(
	.param .u64 cast_i64_u32_param_0,
	.param .u64 cast_i64_u32_param_1,
	.param .u64 cast_i64_u32_param_2,
	.param .u64 cast_i64_u32_param_3,
	.param .u64 cast_i64_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [cast_i64_u32_param_0];
	ld.param.u64 	%rd25, [cast_i64_u32_param_1];
	ld.param.u64 	%rd26, [cast_i64_u32_param_2];
	ld.param.u64 	%rd27, [cast_i64_u32_param_3];
	ld.param.u64 	%rd28, [cast_i64_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB27_5;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB27_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB27_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd62, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB27_5;

$L__BB27_4:
	mul.lo.s64 	%rd62, %rd6, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB27_2;

$L__BB27_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p19 bra 	$L__BB27_15;
	bra.uni 	$L__BB27_6;

$L__BB27_15:
	setp.ge.u64 	%p17, %rd63, %rd24;
	@%p17 bra 	$L__BB27_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB27_17:
	shl.b64 	%rd57, %rd63, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	shl.b64 	%rd60, %rd63, 2;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.u32 	[%rd61], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB27_17;
	bra.uni 	$L__BB27_18;

$L__BB27_6:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB27_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB27_14;

$L__BB27_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB27_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB27_11;

	div.u64 	%rd64, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd64, %rd12;
	sub.s64 	%rd65, %rd10, %rd43;
	bra.uni 	$L__BB27_12;

$L__BB27_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd64, %r29;
	cvt.u64.u32 	%rd65, %r31;

$L__BB27_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd65;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB27_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	shl.b64 	%rd52, %rd63, 2;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u32 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd24;
	@%p15 bra 	$L__BB27_8;
	bra.uni 	$L__BB27_18;

$L__BB27_14:
	ld.global.u64 	%rd54, [%rd2];
	shl.b64 	%rd55, %rd63, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u32 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB27_14;

$L__BB27_18:
	ret;

}
	// .globl	cast_i64_u8
.visible .entry cast_i64_u8(
	.param .u64 cast_i64_u8_param_0,
	.param .u64 cast_i64_u8_param_1,
	.param .u64 cast_i64_u8_param_2,
	.param .u64 cast_i64_u8_param_3,
	.param .u64 cast_i64_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_i64_u8_param_0];
	ld.param.u64 	%rd25, [cast_i64_u8_param_1];
	ld.param.u64 	%rd26, [cast_i64_u8_param_2];
	ld.param.u64 	%rd27, [cast_i64_u8_param_3];
	ld.param.u64 	%rd28, [cast_i64_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB28_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB28_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB28_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB28_5;

$L__BB28_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB28_2;

$L__BB28_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB28_15;
	bra.uni 	$L__BB28_6;

$L__BB28_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB28_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB28_17:
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	add.s64 	%rd58, %rd1, %rd60;
	st.global.u8 	[%rd58], %rd57;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB28_17;
	bra.uni 	$L__BB28_18;

$L__BB28_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB28_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB28_14;

$L__BB28_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB28_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB28_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB28_12;

$L__BB28_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB28_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB28_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	add.s64 	%rd52, %rd1, %rd60;
	st.global.u8 	[%rd52], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB28_8;
	bra.uni 	$L__BB28_18;

$L__BB28_14:
	ld.global.u64 	%rd53, [%rd2];
	add.s64 	%rd54, %rd1, %rd60;
	st.global.u8 	[%rd54], %rd53;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB28_14;

$L__BB28_18:
	ret;

}
	// .globl	cast_i64_i64
.visible .entry cast_i64_i64(
	.param .u64 cast_i64_i64_param_0,
	.param .u64 cast_i64_i64_param_1,
	.param .u64 cast_i64_i64_param_2,
	.param .u64 cast_i64_i64_param_3,
	.param .u64 cast_i64_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd24, [cast_i64_i64_param_0];
	ld.param.u64 	%rd25, [cast_i64_i64_param_1];
	ld.param.u64 	%rd26, [cast_i64_i64_param_2];
	ld.param.u64 	%rd27, [cast_i64_i64_param_3];
	ld.param.u64 	%rd28, [cast_i64_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB29_5;

	mov.u64 	%rd61, 1;
	mov.u32 	%r34, 0;

$L__BB29_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB29_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd61, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB29_5;

$L__BB29_4:
	mul.lo.s64 	%rd61, %rd6, %rd61;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB29_2;

$L__BB29_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd62, %r35;
	@%p19 bra 	$L__BB29_15;
	bra.uni 	$L__BB29_6;

$L__BB29_15:
	setp.ge.u64 	%p17, %rd62, %rd24;
	@%p17 bra 	$L__BB29_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB29_17:
	shl.b64 	%rd57, %rd62, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	add.s64 	%rd60, %rd1, %rd57;
	st.global.u64 	[%rd60], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p18, %rd62, %rd24;
	@%p18 bra 	$L__BB29_17;
	bra.uni 	$L__BB29_18;

$L__BB29_6:
	setp.ge.u64 	%p11, %rd62, %rd24;
	@%p11 bra 	$L__BB29_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB29_14;

$L__BB29_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB29_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB29_11;

	div.u64 	%rd63, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd63, %rd12;
	sub.s64 	%rd64, %rd10, %rd43;
	bra.uni 	$L__BB29_12;

$L__BB29_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd63, %r29;
	cvt.u64.u32 	%rd64, %r31;

$L__BB29_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd64;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB29_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	shl.b64 	%rd52, %rd62, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p15, %rd62, %rd24;
	@%p15 bra 	$L__BB29_8;
	bra.uni 	$L__BB29_18;

$L__BB29_14:
	ld.global.u64 	%rd54, [%rd2];
	shl.b64 	%rd55, %rd62, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p16, %rd62, %rd24;
	@%p16 bra 	$L__BB29_14;

$L__BB29_18:
	ret;

}
	// .globl	cast_i64_f32
.visible .entry cast_i64_f32(
	.param .u64 cast_i64_f32_param_0,
	.param .u64 cast_i64_f32_param_1,
	.param .u64 cast_i64_f32_param_2,
	.param .u64 cast_i64_f32_param_3,
	.param .u64 cast_i64_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [cast_i64_f32_param_0];
	ld.param.u64 	%rd25, [cast_i64_f32_param_1];
	ld.param.u64 	%rd26, [cast_i64_f32_param_2];
	ld.param.u64 	%rd27, [cast_i64_f32_param_3];
	ld.param.u64 	%rd28, [cast_i64_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB30_5;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB30_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB30_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd62, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB30_5;

$L__BB30_4:
	mul.lo.s64 	%rd62, %rd6, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB30_2;

$L__BB30_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p19 bra 	$L__BB30_15;
	bra.uni 	$L__BB30_6;

$L__BB30_15:
	setp.ge.u64 	%p17, %rd63, %rd24;
	@%p17 bra 	$L__BB30_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB30_17:
	shl.b64 	%rd57, %rd63, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	cvt.rn.f32.s64 	%f3, %rd59;
	shl.b64 	%rd60, %rd63, 2;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.f32 	[%rd61], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB30_17;
	bra.uni 	$L__BB30_18;

$L__BB30_6:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB30_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB30_14;

$L__BB30_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB30_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB30_11;

	div.u64 	%rd64, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd64, %rd12;
	sub.s64 	%rd65, %rd10, %rd43;
	bra.uni 	$L__BB30_12;

$L__BB30_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd64, %r29;
	cvt.u64.u32 	%rd65, %r31;

$L__BB30_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd65;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB30_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	cvt.rn.f32.s64 	%f1, %rd51;
	shl.b64 	%rd52, %rd63, 2;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.f32 	[%rd53], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd24;
	@%p15 bra 	$L__BB30_8;
	bra.uni 	$L__BB30_18;

$L__BB30_14:
	ld.global.u64 	%rd54, [%rd2];
	cvt.rn.f32.s64 	%f2, %rd54;
	shl.b64 	%rd55, %rd63, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.f32 	[%rd56], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB30_14;

$L__BB30_18:
	ret;

}
	// .globl	cast_i64_f64
.visible .entry cast_i64_f64(
	.param .u64 cast_i64_f64_param_0,
	.param .u64 cast_i64_f64_param_1,
	.param .u64 cast_i64_f64_param_2,
	.param .u64 cast_i64_f64_param_3,
	.param .u64 cast_i64_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd24, [cast_i64_f64_param_0];
	ld.param.u64 	%rd25, [cast_i64_f64_param_1];
	ld.param.u64 	%rd26, [cast_i64_f64_param_2];
	ld.param.u64 	%rd27, [cast_i64_f64_param_3];
	ld.param.u64 	%rd28, [cast_i64_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB31_5;

	mov.u64 	%rd61, 1;
	mov.u32 	%r34, 0;

$L__BB31_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB31_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd61, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB31_5;

$L__BB31_4:
	mul.lo.s64 	%rd61, %rd6, %rd61;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB31_2;

$L__BB31_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd62, %r35;
	@%p19 bra 	$L__BB31_15;
	bra.uni 	$L__BB31_6;

$L__BB31_15:
	setp.ge.u64 	%p17, %rd62, %rd24;
	@%p17 bra 	$L__BB31_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB31_17:
	shl.b64 	%rd57, %rd62, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	cvt.rn.f64.s64 	%fd3, %rd59;
	add.s64 	%rd60, %rd1, %rd57;
	st.global.f64 	[%rd60], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p18, %rd62, %rd24;
	@%p18 bra 	$L__BB31_17;
	bra.uni 	$L__BB31_18;

$L__BB31_6:
	setp.ge.u64 	%p11, %rd62, %rd24;
	@%p11 bra 	$L__BB31_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB31_14;

$L__BB31_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB31_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB31_11;

	div.u64 	%rd63, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd63, %rd12;
	sub.s64 	%rd64, %rd10, %rd43;
	bra.uni 	$L__BB31_12;

$L__BB31_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd63, %r29;
	cvt.u64.u32 	%rd64, %r31;

$L__BB31_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd64;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB31_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	cvt.rn.f64.s64 	%fd1, %rd51;
	shl.b64 	%rd52, %rd62, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.f64 	[%rd53], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p15, %rd62, %rd24;
	@%p15 bra 	$L__BB31_8;
	bra.uni 	$L__BB31_18;

$L__BB31_14:
	ld.global.u64 	%rd54, [%rd2];
	cvt.rn.f64.s64 	%fd2, %rd54;
	shl.b64 	%rd55, %rd62, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.f64 	[%rd56], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p16, %rd62, %rd24;
	@%p16 bra 	$L__BB31_14;

$L__BB31_18:
	ret;

}
	// .globl	cast_f32_u8
.visible .entry cast_f32_u8(
	.param .u64 cast_f32_u8_param_0,
	.param .u64 cast_f32_u8_param_1,
	.param .u64 cast_f32_u8_param_2,
	.param .u64 cast_f32_u8_param_3,
	.param .u64 cast_f32_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd24, [cast_f32_u8_param_0];
	ld.param.u64 	%rd25, [cast_f32_u8_param_1];
	ld.param.u64 	%rd26, [cast_f32_u8_param_2];
	ld.param.u64 	%rd27, [cast_f32_u8_param_3];
	ld.param.u64 	%rd28, [cast_f32_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB32_5;

	mov.u64 	%rd56, 1;
	mov.u32 	%r37, 0;

$L__BB32_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB32_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd56, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB32_5;

$L__BB32_4:
	mul.lo.s64 	%rd56, %rd6, %rd56;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB32_2;

$L__BB32_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r38;
	@%p19 bra 	$L__BB32_15;
	bra.uni 	$L__BB32_6;

$L__BB32_15:
	setp.ge.u64 	%p17, %rd57, %rd24;
	@%p17 bra 	$L__BB32_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB32_17:
	shl.b64 	%rd53, %rd57, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.f32 	%f3, [%rd54];
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p18, %rd57, %rd24;
	@%p18 bra 	$L__BB32_17;
	bra.uni 	$L__BB32_18;

$L__BB32_6:
	setp.ge.u64 	%p11, %rd57, %rd24;
	@%p11 bra 	$L__BB32_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB32_14;

$L__BB32_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB32_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB32_11;

	div.u64 	%rd58, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd58, %rd12;
	sub.s64 	%rd59, %rd10, %rd43;
	bra.uni 	$L__BB32_12;

$L__BB32_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd58, %r29;
	cvt.u64.u32 	%rd59, %r31;

$L__BB32_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd59;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB32_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	cvt.rzi.u32.f32 	%r33, %f1;
	add.s64 	%rd51, %rd1, %rd57;
	st.global.u8 	[%rd51], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p15, %rd57, %rd24;
	@%p15 bra 	$L__BB32_8;
	bra.uni 	$L__BB32_18;

$L__BB32_14:
	ld.global.f32 	%f2, [%rd2];
	cvt.rzi.u32.f32 	%r34, %f2;
	add.s64 	%rd52, %rd1, %rd57;
	st.global.u8 	[%rd52], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p16, %rd57, %rd24;
	@%p16 bra 	$L__BB32_14;

$L__BB32_18:
	ret;

}
	// .globl	cast_f32_u32
.visible .entry cast_f32_u32(
	.param .u64 cast_f32_u32_param_0,
	.param .u64 cast_f32_u32_param_1,
	.param .u64 cast_f32_u32_param_2,
	.param .u64 cast_f32_u32_param_3,
	.param .u64 cast_f32_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_f32_u32_param_0];
	ld.param.u64 	%rd25, [cast_f32_u32_param_1];
	ld.param.u64 	%rd26, [cast_f32_u32_param_2];
	ld.param.u64 	%rd27, [cast_f32_u32_param_3];
	ld.param.u64 	%rd28, [cast_f32_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB33_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r37, 0;

$L__BB33_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB33_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB33_5;

$L__BB33_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB33_2;

$L__BB33_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r38;
	@%p19 bra 	$L__BB33_15;
	bra.uni 	$L__BB33_6;

$L__BB33_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB33_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB33_17:
	shl.b64 	%rd55, %rd59, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f3, [%rd56];
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd57, %rd1, %rd55;
	st.global.u32 	[%rd57], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB33_17;
	bra.uni 	$L__BB33_18;

$L__BB33_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB33_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB33_14;

$L__BB33_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB33_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB33_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB33_12;

$L__BB33_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB33_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd60;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB33_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	cvt.rzi.u32.f32 	%r33, %f1;
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB33_8;
	bra.uni 	$L__BB33_18;

$L__BB33_14:
	ld.global.f32 	%f2, [%rd2];
	cvt.rzi.u32.f32 	%r34, %f2;
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB33_14;

$L__BB33_18:
	ret;

}
	// .globl	cast_f32_i64
.visible .entry cast_f32_i64(
	.param .u64 cast_f32_i64_param_0,
	.param .u64 cast_f32_i64_param_1,
	.param .u64 cast_f32_i64_param_2,
	.param .u64 cast_f32_i64_param_3,
	.param .u64 cast_f32_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [cast_f32_i64_param_0];
	ld.param.u64 	%rd25, [cast_f32_i64_param_1];
	ld.param.u64 	%rd26, [cast_f32_i64_param_2];
	ld.param.u64 	%rd27, [cast_f32_i64_param_3];
	ld.param.u64 	%rd28, [cast_f32_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB34_5;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB34_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB34_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd62, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB34_5;

$L__BB34_4:
	mul.lo.s64 	%rd62, %rd6, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB34_2;

$L__BB34_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p19 bra 	$L__BB34_15;
	bra.uni 	$L__BB34_6;

$L__BB34_15:
	setp.ge.u64 	%p17, %rd63, %rd24;
	@%p17 bra 	$L__BB34_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB34_17:
	shl.b64 	%rd57, %rd63, 2;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.f32 	%f3, [%rd58];
	cvt.rzi.s64.f32 	%rd59, %f3;
	shl.b64 	%rd60, %rd63, 3;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.u64 	[%rd61], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB34_17;
	bra.uni 	$L__BB34_18;

$L__BB34_6:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB34_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB34_14;

$L__BB34_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB34_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB34_11;

	div.u64 	%rd64, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd64, %rd12;
	sub.s64 	%rd65, %rd10, %rd43;
	bra.uni 	$L__BB34_12;

$L__BB34_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd64, %r29;
	cvt.u64.u32 	%rd65, %r31;

$L__BB34_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd65;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB34_9;

	mul.wide.u32 	%rd49, %r38, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	cvt.rzi.s64.f32 	%rd51, %f1;
	shl.b64 	%rd52, %rd63, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd24;
	@%p15 bra 	$L__BB34_8;
	bra.uni 	$L__BB34_18;

$L__BB34_14:
	ld.global.f32 	%f2, [%rd2];
	cvt.rzi.s64.f32 	%rd54, %f2;
	shl.b64 	%rd55, %rd63, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB34_14;

$L__BB34_18:
	ret;

}
	// .globl	cast_f32_f32
.visible .entry cast_f32_f32(
	.param .u64 cast_f32_f32_param_0,
	.param .u64 cast_f32_f32_param_1,
	.param .u64 cast_f32_f32_param_2,
	.param .u64 cast_f32_f32_param_3,
	.param .u64 cast_f32_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_f32_f32_param_0];
	ld.param.u64 	%rd25, [cast_f32_f32_param_1];
	ld.param.u64 	%rd26, [cast_f32_f32_param_2];
	ld.param.u64 	%rd27, [cast_f32_f32_param_3];
	ld.param.u64 	%rd28, [cast_f32_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB35_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB35_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB35_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB35_5;

$L__BB35_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB35_2;

$L__BB35_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB35_15;
	bra.uni 	$L__BB35_6;

$L__BB35_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB35_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB35_17:
	shl.b64 	%rd55, %rd59, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f3, [%rd56];
	add.s64 	%rd57, %rd1, %rd55;
	st.global.f32 	[%rd57], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB35_17;
	bra.uni 	$L__BB35_18;

$L__BB35_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB35_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB35_14;

$L__BB35_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB35_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB35_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB35_12;

$L__BB35_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB35_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB35_9;

	mul.wide.u32 	%rd49, %r38, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB35_8;
	bra.uni 	$L__BB35_18;

$L__BB35_14:
	ld.global.f32 	%f2, [%rd2];
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB35_14;

$L__BB35_18:
	ret;

}
	// .globl	cast_f32_f64
.visible .entry cast_f32_f64(
	.param .u64 cast_f32_f64_param_0,
	.param .u64 cast_f32_f64_param_1,
	.param .u64 cast_f32_f64_param_2,
	.param .u64 cast_f32_f64_param_3,
	.param .u64 cast_f32_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f32_f64_param_0];
	ld.param.u64 	%rd25, [cast_f32_f64_param_1];
	ld.param.u64 	%rd26, [cast_f32_f64_param_2];
	ld.param.u64 	%rd27, [cast_f32_f64_param_3];
	ld.param.u64 	%rd28, [cast_f32_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB36_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB36_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB36_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB36_5;

$L__BB36_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB36_2;

$L__BB36_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB36_15;
	bra.uni 	$L__BB36_6;

$L__BB36_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB36_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB36_17:
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f3, [%rd56];
	cvt.f64.f32 	%fd3, %f3;
	shl.b64 	%rd57, %rd60, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB36_17;
	bra.uni 	$L__BB36_18;

$L__BB36_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB36_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB36_14;

$L__BB36_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB36_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB36_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB36_12;

$L__BB36_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB36_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB36_9;

	mul.wide.u32 	%rd49, %r38, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	cvt.f64.f32 	%fd1, %f1;
	shl.b64 	%rd51, %rd60, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB36_8;
	bra.uni 	$L__BB36_18;

$L__BB36_14:
	ld.global.f32 	%f2, [%rd2];
	cvt.f64.f32 	%fd2, %f2;
	shl.b64 	%rd53, %rd60, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB36_14;

$L__BB36_18:
	ret;

}
	// .globl	cast_f64_u8
.visible .entry cast_f64_u8(
	.param .u64 cast_f64_u8_param_0,
	.param .u64 cast_f64_u8_param_1,
	.param .u64 cast_f64_u8_param_2,
	.param .u64 cast_f64_u8_param_3,
	.param .u64 cast_f64_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd24, [cast_f64_u8_param_0];
	ld.param.u64 	%rd25, [cast_f64_u8_param_1];
	ld.param.u64 	%rd26, [cast_f64_u8_param_2];
	ld.param.u64 	%rd27, [cast_f64_u8_param_3];
	ld.param.u64 	%rd28, [cast_f64_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB37_5;

	mov.u64 	%rd56, 1;
	mov.u32 	%r37, 0;

$L__BB37_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB37_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd56, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB37_5;

$L__BB37_4:
	mul.lo.s64 	%rd56, %rd6, %rd56;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB37_2;

$L__BB37_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r38;
	@%p19 bra 	$L__BB37_15;
	bra.uni 	$L__BB37_6;

$L__BB37_15:
	setp.ge.u64 	%p17, %rd57, %rd24;
	@%p17 bra 	$L__BB37_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB37_17:
	shl.b64 	%rd53, %rd57, 3;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.f64 	%fd3, [%rd54];
	cvt.rzi.u32.f64 	%r36, %fd3;
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p18, %rd57, %rd24;
	@%p18 bra 	$L__BB37_17;
	bra.uni 	$L__BB37_18;

$L__BB37_6:
	setp.ge.u64 	%p11, %rd57, %rd24;
	@%p11 bra 	$L__BB37_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB37_14;

$L__BB37_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB37_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB37_11;

	div.u64 	%rd58, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd58, %rd12;
	sub.s64 	%rd59, %rd10, %rd43;
	bra.uni 	$L__BB37_12;

$L__BB37_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd58, %r29;
	cvt.u64.u32 	%rd59, %r31;

$L__BB37_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd59;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB37_9;

	mul.wide.u32 	%rd49, %r41, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rzi.u32.f64 	%r33, %fd1;
	add.s64 	%rd51, %rd1, %rd57;
	st.global.u8 	[%rd51], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p15, %rd57, %rd24;
	@%p15 bra 	$L__BB37_8;
	bra.uni 	$L__BB37_18;

$L__BB37_14:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rzi.u32.f64 	%r34, %fd2;
	add.s64 	%rd52, %rd1, %rd57;
	st.global.u8 	[%rd52], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p16, %rd57, %rd24;
	@%p16 bra 	$L__BB37_14;

$L__BB37_18:
	ret;

}
	// .globl	cast_f64_u32
.visible .entry cast_f64_u32(
	.param .u64 cast_f64_u32_param_0,
	.param .u64 cast_f64_u32_param_1,
	.param .u64 cast_f64_u32_param_2,
	.param .u64 cast_f64_u32_param_3,
	.param .u64 cast_f64_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f64_u32_param_0];
	ld.param.u64 	%rd25, [cast_f64_u32_param_1];
	ld.param.u64 	%rd26, [cast_f64_u32_param_2];
	ld.param.u64 	%rd27, [cast_f64_u32_param_3];
	ld.param.u64 	%rd28, [cast_f64_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB38_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB38_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB38_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB38_5;

$L__BB38_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB38_2;

$L__BB38_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p19 bra 	$L__BB38_15;
	bra.uni 	$L__BB38_6;

$L__BB38_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB38_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB38_17:
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd3, [%rd56];
	cvt.rzi.u32.f64 	%r36, %fd3;
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u32 	[%rd58], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB38_17;
	bra.uni 	$L__BB38_18;

$L__BB38_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB38_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB38_14;

$L__BB38_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB38_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB38_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB38_12;

$L__BB38_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB38_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd61;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB38_9;

	mul.wide.u32 	%rd49, %r41, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rzi.u32.f64 	%r33, %fd1;
	shl.b64 	%rd51, %rd60, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB38_8;
	bra.uni 	$L__BB38_18;

$L__BB38_14:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rzi.u32.f64 	%r34, %fd2;
	shl.b64 	%rd53, %rd60, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB38_14;

$L__BB38_18:
	ret;

}
	// .globl	cast_f64_i64
.visible .entry cast_f64_i64(
	.param .u64 cast_f64_i64_param_0,
	.param .u64 cast_f64_i64_param_1,
	.param .u64 cast_f64_i64_param_2,
	.param .u64 cast_f64_i64_param_3,
	.param .u64 cast_f64_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd24, [cast_f64_i64_param_0];
	ld.param.u64 	%rd25, [cast_f64_i64_param_1];
	ld.param.u64 	%rd26, [cast_f64_i64_param_2];
	ld.param.u64 	%rd27, [cast_f64_i64_param_3];
	ld.param.u64 	%rd28, [cast_f64_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB39_5;

	mov.u64 	%rd61, 1;
	mov.u32 	%r34, 0;

$L__BB39_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB39_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd61, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB39_5;

$L__BB39_4:
	mul.lo.s64 	%rd61, %rd6, %rd61;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB39_2;

$L__BB39_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd62, %r35;
	@%p19 bra 	$L__BB39_15;
	bra.uni 	$L__BB39_6;

$L__BB39_15:
	setp.ge.u64 	%p17, %rd62, %rd24;
	@%p17 bra 	$L__BB39_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB39_17:
	shl.b64 	%rd57, %rd62, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.f64 	%fd3, [%rd58];
	cvt.rzi.s64.f64 	%rd59, %fd3;
	add.s64 	%rd60, %rd1, %rd57;
	st.global.u64 	[%rd60], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p18, %rd62, %rd24;
	@%p18 bra 	$L__BB39_17;
	bra.uni 	$L__BB39_18;

$L__BB39_6:
	setp.ge.u64 	%p11, %rd62, %rd24;
	@%p11 bra 	$L__BB39_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB39_14;

$L__BB39_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB39_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB39_11;

	div.u64 	%rd63, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd63, %rd12;
	sub.s64 	%rd64, %rd10, %rd43;
	bra.uni 	$L__BB39_12;

$L__BB39_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd63, %r29;
	cvt.u64.u32 	%rd64, %r31;

$L__BB39_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd64;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB39_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rzi.s64.f64 	%rd51, %fd1;
	shl.b64 	%rd52, %rd62, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p15, %rd62, %rd24;
	@%p15 bra 	$L__BB39_8;
	bra.uni 	$L__BB39_18;

$L__BB39_14:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rzi.s64.f64 	%rd54, %fd2;
	shl.b64 	%rd55, %rd62, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p16, %rd62, %rd24;
	@%p16 bra 	$L__BB39_14;

$L__BB39_18:
	ret;

}
	// .globl	cast_f64_f32
.visible .entry cast_f64_f32(
	.param .u64 cast_f64_f32_param_0,
	.param .u64 cast_f64_f32_param_1,
	.param .u64 cast_f64_f32_param_2,
	.param .u64 cast_f64_f32_param_3,
	.param .u64 cast_f64_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f64_f32_param_0];
	ld.param.u64 	%rd25, [cast_f64_f32_param_1];
	ld.param.u64 	%rd26, [cast_f64_f32_param_2];
	ld.param.u64 	%rd27, [cast_f64_f32_param_3];
	ld.param.u64 	%rd28, [cast_f64_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB40_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB40_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB40_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB40_5;

$L__BB40_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB40_2;

$L__BB40_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB40_15;
	bra.uni 	$L__BB40_6;

$L__BB40_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB40_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB40_17:
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd3, [%rd56];
	cvt.rn.f32.f64 	%f3, %fd3;
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f32 	[%rd58], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB40_17;
	bra.uni 	$L__BB40_18;

$L__BB40_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB40_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB40_14;

$L__BB40_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB40_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB40_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB40_12;

$L__BB40_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB40_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB40_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rn.f32.f64 	%f1, %fd1;
	shl.b64 	%rd51, %rd60, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB40_8;
	bra.uni 	$L__BB40_18;

$L__BB40_14:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rn.f32.f64 	%f2, %fd2;
	shl.b64 	%rd53, %rd60, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB40_14;

$L__BB40_18:
	ret;

}
	// .globl	cast_f64_f64
.visible .entry cast_f64_f64(
	.param .u64 cast_f64_f64_param_0,
	.param .u64 cast_f64_f64_param_1,
	.param .u64 cast_f64_f64_param_2,
	.param .u64 cast_f64_f64_param_3,
	.param .u64 cast_f64_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_f64_f64_param_0];
	ld.param.u64 	%rd25, [cast_f64_f64_param_1];
	ld.param.u64 	%rd26, [cast_f64_f64_param_2];
	ld.param.u64 	%rd27, [cast_f64_f64_param_3];
	ld.param.u64 	%rd28, [cast_f64_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB41_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB41_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB41_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB41_5;

$L__BB41_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB41_2;

$L__BB41_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB41_15;
	bra.uni 	$L__BB41_6;

$L__BB41_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB41_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB41_17:
	shl.b64 	%rd55, %rd59, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd3, [%rd56];
	add.s64 	%rd57, %rd1, %rd55;
	st.global.f64 	[%rd57], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB41_17;
	bra.uni 	$L__BB41_18;

$L__BB41_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB41_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB41_14;

$L__BB41_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB41_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB41_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB41_12;

$L__BB41_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB41_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB41_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	shl.b64 	%rd51, %rd59, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB41_8;
	bra.uni 	$L__BB41_18;

$L__BB41_14:
	ld.global.f64 	%fd2, [%rd2];
	shl.b64 	%rd53, %rd59, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB41_14;

$L__BB41_18:
	ret;

}

