//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34385749
// Cuda compilation tools, release 12.5, V12.5.82
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_61
.address_size 64

	// .globl	affine_f16

.visible .entry affine_f16(
	.param .u64 affine_f16_param_0,
	.param .u64 affine_f16_param_1,
	.param .u64 affine_f16_param_2,
	.param .u64 affine_f16_param_3,
	.param .u64 affine_f16_param_4,
	.param .align 2 .b8 affine_f16_param_5[2],
	.param .align 2 .b8 affine_f16_param_6[2]
)
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<29>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<71>;


	ld.param.u16 	%rs4, [affine_f16_param_6];
	ld.param.u16 	%rs3, [affine_f16_param_5];
	ld.param.u64 	%rd27, [affine_f16_param_0];
	ld.param.u64 	%rd28, [affine_f16_param_1];
	ld.param.u64 	%rd30, [affine_f16_param_2];
	ld.param.u64 	%rd29, [affine_f16_param_3];
	ld.param.u64 	%rd31, [affine_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd29;
	cvta.to.global.u64 	%rd3, %rd30;
	setp.eq.s64 	%p3, %rd28, 0;
	setp.eq.s64 	%p4, %rd30, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p23, %p2;
	@%p5 bra 	$L__BB0_5;

	mov.u64 	%rd64, 1;
	mov.u32 	%r36, 0;

$L__BB0_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	shl.b64 	%rd35, %rd34, 3;
	and.b64  	%rd36, %rd35, 34359738360;
	add.s64 	%rd5, %rd3, %rd36;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB0_4;

	shl.b64 	%rd37, %rd28, 3;
	add.s64 	%rd38, %rd5, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p8, %rd64, %rd39;
	mov.pred 	%p23, 0;
	@%p8 bra 	$L__BB0_5;

$L__BB0_4:
	mul.lo.s64 	%rd64, %rd6, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd40, %r36;
	setp.lt.u64 	%p10, %rd40, %rd28;
	mov.pred 	%p23, %p2;
	@%p10 bra 	$L__BB0_2;

$L__BB0_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd8, %r4;
	@%p23 bra 	$L__BB0_17;
	bra.uni 	$L__BB0_6;

$L__BB0_17:
	setp.ge.u64 	%p20, %rd8, %rd27;
	@%p20 bra 	$L__BB0_20;

	setp.eq.s64 	%p21, %rd29, 0;
	selp.b64 	%rd24, %rd1, %rd2, %p21;
	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r35;

$L__BB0_19:
	shl.b64 	%rd61, %rd8, 1;
	add.s64 	%rd62, %rd24, %rd61;
	ld.global.u16 	%rs24, [%rd62];
	// begin inline asm
	{mul.f16 %rs23,%rs24,%rs3;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs26,%rs23,%rs4;
}
	// end inline asm
	add.s64 	%rd63, %rd1, %rd61;
	st.global.u16 	[%rd63], %rs26;
	add.s32 	%r4, %r4, %r18;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p22, %rd8, %rd27;
	@%p22 bra 	$L__BB0_19;
	bra.uni 	$L__BB0_20;

$L__BB0_6:
	setp.ge.u64 	%p11, %rd8, %rd27;
	@%p11 bra 	$L__BB0_20;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p3 bra 	$L__BB0_14;

$L__BB0_8:
	mov.u32 	%r38, 0;
	mov.u32 	%r39, %r4;
	mov.u32 	%r40, %r38;

$L__BB0_9:
	not.b32 	%r28, %r38;
	cvt.u64.u32 	%rd41, %r28;
	add.s64 	%rd42, %rd41, %rd28;
	cvt.u64.u32 	%rd10, %r39;
	shl.b64 	%rd43, %rd42, 3;
	and.b64  	%rd44, %rd43, 34359738360;
	add.s64 	%rd11, %rd3, %rd44;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd45, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd45, 0;
	@%p13 bra 	$L__BB0_11;

	div.u64 	%rd66, %rd10, %rd12;
	mul.lo.s64 	%rd46, %rd66, %rd12;
	sub.s64 	%rd67, %rd10, %rd46;
	bra.uni 	$L__BB0_12;

$L__BB0_11:
	cvt.u32.u64 	%r29, %rd12;
	cvt.u32.u64 	%r30, %rd10;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd66, %r31;
	cvt.u64.u32 	%rd67, %r33;

$L__BB0_12:
	shl.b64 	%rd47, %rd28, 3;
	add.s64 	%rd48, %rd11, %rd47;
	ld.global.u64 	%rd49, [%rd48];
	mul.lo.s64 	%rd50, %rd49, %rd67;
	cvt.u32.u64 	%r34, %rd50;
	add.s32 	%r40, %r40, %r34;
	cvt.u32.u64 	%r39, %rd66;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd51, %r38;
	setp.lt.u64 	%p14, %rd51, %rd28;
	@%p14 bra 	$L__BB0_9;

	setp.eq.s64 	%p15, %rd29, 0;
	mul.wide.u32 	%rd52, %r40, 2;
	add.s64 	%rd53, %rd2, %rd52;
	shl.b64 	%rd54, %rd8, 1;
	add.s64 	%rd55, %rd1, %rd54;
	selp.b64 	%rd56, %rd55, %rd53, %p15;
	ld.global.u16 	%rs6, [%rd56];
	// begin inline asm
	{mul.f16 %rs5,%rs6,%rs3;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs8,%rs5,%rs4;
}
	// end inline asm
	st.global.u16 	[%rd55], %rs8;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p16, %rd8, %rd27;
	@%p16 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_20;

$L__BB0_14:
	setp.eq.s64 	%p17, %rd29, 0;
	@%p17 bra 	$L__BB0_16;

$L__BB0_15:
	ld.global.u16 	%rs12, [%rd2];
	// begin inline asm
	{mul.f16 %rs11,%rs12,%rs3;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs14,%rs11,%rs4;
}
	// end inline asm
	shl.b64 	%rd57, %rd8, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs14;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p18, %rd8, %rd27;
	@%p18 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_20;

$L__BB0_16:
	shl.b64 	%rd59, %rd8, 1;
	add.s64 	%rd60, %rd1, %rd59;
	ld.global.u16 	%rs18, [%rd60];
	// begin inline asm
	{mul.f16 %rs17,%rs18,%rs3;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs20,%rs17,%rs4;
}
	// end inline asm
	st.global.u16 	[%rd60], %rs20;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p19, %rd8, %rd27;
	@%p19 bra 	$L__BB0_16;

$L__BB0_20:
	ret;

}
	// .globl	affine_f32
.visible .entry affine_f32(
	.param .u64 affine_f32_param_0,
	.param .u64 affine_f32_param_1,
	.param .u64 affine_f32_param_2,
	.param .u64 affine_f32_param_3,
	.param .u64 affine_f32_param_4,
	.param .f32 affine_f32_param_5,
	.param .f32 affine_f32_param_6
)
{
	.reg .pred 	%p<24>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<71>;


	ld.param.u64 	%rd27, [affine_f32_param_0];
	ld.param.u64 	%rd28, [affine_f32_param_1];
	ld.param.u64 	%rd30, [affine_f32_param_2];
	ld.param.u64 	%rd29, [affine_f32_param_3];
	ld.param.u64 	%rd31, [affine_f32_param_4];
	ld.param.f32 	%f1, [affine_f32_param_5];
	ld.param.f32 	%f2, [affine_f32_param_6];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd29;
	cvta.to.global.u64 	%rd3, %rd30;
	setp.eq.s64 	%p3, %rd28, 0;
	setp.eq.s64 	%p4, %rd30, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p23, %p2;
	@%p5 bra 	$L__BB1_5;

	mov.u64 	%rd64, 1;
	mov.u32 	%r36, 0;

$L__BB1_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	shl.b64 	%rd35, %rd34, 3;
	and.b64  	%rd36, %rd35, 34359738360;
	add.s64 	%rd5, %rd3, %rd36;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB1_4;

	shl.b64 	%rd37, %rd28, 3;
	add.s64 	%rd38, %rd5, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p8, %rd64, %rd39;
	mov.pred 	%p23, 0;
	@%p8 bra 	$L__BB1_5;

$L__BB1_4:
	mul.lo.s64 	%rd64, %rd6, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd40, %r36;
	setp.lt.u64 	%p10, %rd40, %rd28;
	mov.pred 	%p23, %p2;
	@%p10 bra 	$L__BB1_2;

$L__BB1_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd8, %r4;
	@%p23 bra 	$L__BB1_17;
	bra.uni 	$L__BB1_6;

$L__BB1_17:
	setp.ge.u64 	%p20, %rd8, %rd27;
	@%p20 bra 	$L__BB1_20;

	setp.eq.s64 	%p21, %rd29, 0;
	selp.b64 	%rd24, %rd1, %rd2, %p21;
	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r35;

$L__BB1_19:
	shl.b64 	%rd61, %rd8, 2;
	add.s64 	%rd62, %rd24, %rd61;
	ld.global.f32 	%f9, [%rd62];
	fma.rn.f32 	%f10, %f9, %f1, %f2;
	add.s64 	%rd63, %rd1, %rd61;
	st.global.f32 	[%rd63], %f10;
	add.s32 	%r4, %r4, %r18;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p22, %rd8, %rd27;
	@%p22 bra 	$L__BB1_19;
	bra.uni 	$L__BB1_20;

$L__BB1_6:
	setp.ge.u64 	%p11, %rd8, %rd27;
	@%p11 bra 	$L__BB1_20;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p3 bra 	$L__BB1_14;

$L__BB1_8:
	mov.u32 	%r38, 0;
	mov.u32 	%r39, %r4;
	mov.u32 	%r40, %r38;

$L__BB1_9:
	not.b32 	%r28, %r38;
	cvt.u64.u32 	%rd41, %r28;
	add.s64 	%rd42, %rd41, %rd28;
	cvt.u64.u32 	%rd10, %r39;
	shl.b64 	%rd43, %rd42, 3;
	and.b64  	%rd44, %rd43, 34359738360;
	add.s64 	%rd11, %rd3, %rd44;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd45, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd45, 0;
	@%p13 bra 	$L__BB1_11;

	div.u64 	%rd66, %rd10, %rd12;
	mul.lo.s64 	%rd46, %rd66, %rd12;
	sub.s64 	%rd67, %rd10, %rd46;
	bra.uni 	$L__BB1_12;

$L__BB1_11:
	cvt.u32.u64 	%r29, %rd12;
	cvt.u32.u64 	%r30, %rd10;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd66, %r31;
	cvt.u64.u32 	%rd67, %r33;

$L__BB1_12:
	shl.b64 	%rd47, %rd28, 3;
	add.s64 	%rd48, %rd11, %rd47;
	ld.global.u64 	%rd49, [%rd48];
	mul.lo.s64 	%rd50, %rd49, %rd67;
	cvt.u32.u64 	%r34, %rd50;
	add.s32 	%r40, %r40, %r34;
	cvt.u32.u64 	%r39, %rd66;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd51, %r38;
	setp.lt.u64 	%p14, %rd51, %rd28;
	@%p14 bra 	$L__BB1_9;

	setp.eq.s64 	%p15, %rd29, 0;
	mul.wide.u32 	%rd52, %r40, 4;
	add.s64 	%rd53, %rd2, %rd52;
	shl.b64 	%rd54, %rd8, 2;
	add.s64 	%rd55, %rd1, %rd54;
	selp.b64 	%rd56, %rd55, %rd53, %p15;
	ld.global.f32 	%f3, [%rd56];
	fma.rn.f32 	%f4, %f3, %f1, %f2;
	st.global.f32 	[%rd55], %f4;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p16, %rd8, %rd27;
	@%p16 bra 	$L__BB1_8;
	bra.uni 	$L__BB1_20;

$L__BB1_14:
	setp.eq.s64 	%p17, %rd29, 0;
	@%p17 bra 	$L__BB1_16;

$L__BB1_15:
	ld.global.f32 	%f5, [%rd2];
	fma.rn.f32 	%f6, %f5, %f1, %f2;
	shl.b64 	%rd57, %rd8, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f32 	[%rd58], %f6;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p18, %rd8, %rd27;
	@%p18 bra 	$L__BB1_15;
	bra.uni 	$L__BB1_20;

$L__BB1_16:
	shl.b64 	%rd59, %rd8, 2;
	add.s64 	%rd60, %rd1, %rd59;
	ld.global.f32 	%f7, [%rd60];
	fma.rn.f32 	%f8, %f7, %f1, %f2;
	st.global.f32 	[%rd60], %f8;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p19, %rd8, %rd27;
	@%p19 bra 	$L__BB1_16;

$L__BB1_20:
	ret;

}
	// .globl	affine_f64
.visible .entry affine_f64(
	.param .u64 affine_f64_param_0,
	.param .u64 affine_f64_param_1,
	.param .u64 affine_f64_param_2,
	.param .u64 affine_f64_param_3,
	.param .u64 affine_f64_param_4,
	.param .f64 affine_f64_param_5,
	.param .f64 affine_f64_param_6
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<11>;
	.reg .b64 	%rd<71>;


	ld.param.u64 	%rd27, [affine_f64_param_0];
	ld.param.u64 	%rd28, [affine_f64_param_1];
	ld.param.u64 	%rd30, [affine_f64_param_2];
	ld.param.u64 	%rd29, [affine_f64_param_3];
	ld.param.u64 	%rd31, [affine_f64_param_4];
	ld.param.f64 	%fd1, [affine_f64_param_5];
	ld.param.f64 	%fd2, [affine_f64_param_6];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd29;
	cvta.to.global.u64 	%rd3, %rd30;
	setp.eq.s64 	%p3, %rd28, 0;
	setp.eq.s64 	%p4, %rd30, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p23, %p2;
	@%p5 bra 	$L__BB2_5;

	mov.u64 	%rd64, 1;
	mov.u32 	%r36, 0;

$L__BB2_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	shl.b64 	%rd35, %rd34, 3;
	and.b64  	%rd36, %rd35, 34359738360;
	add.s64 	%rd5, %rd3, %rd36;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB2_4;

	shl.b64 	%rd37, %rd28, 3;
	add.s64 	%rd38, %rd5, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p8, %rd64, %rd39;
	mov.pred 	%p23, 0;
	@%p8 bra 	$L__BB2_5;

$L__BB2_4:
	mul.lo.s64 	%rd64, %rd6, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd40, %r36;
	setp.lt.u64 	%p10, %rd40, %rd28;
	mov.pred 	%p23, %p2;
	@%p10 bra 	$L__BB2_2;

$L__BB2_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd8, %r4;
	@%p23 bra 	$L__BB2_17;
	bra.uni 	$L__BB2_6;

$L__BB2_17:
	setp.ge.u64 	%p20, %rd8, %rd27;
	@%p20 bra 	$L__BB2_20;

	setp.eq.s64 	%p21, %rd29, 0;
	selp.b64 	%rd24, %rd1, %rd2, %p21;
	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r35;

$L__BB2_19:
	shl.b64 	%rd61, %rd8, 3;
	add.s64 	%rd62, %rd24, %rd61;
	ld.global.f64 	%fd9, [%rd62];
	fma.rn.f64 	%fd10, %fd9, %fd1, %fd2;
	add.s64 	%rd63, %rd1, %rd61;
	st.global.f64 	[%rd63], %fd10;
	add.s32 	%r4, %r4, %r18;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p22, %rd8, %rd27;
	@%p22 bra 	$L__BB2_19;
	bra.uni 	$L__BB2_20;

$L__BB2_6:
	setp.ge.u64 	%p11, %rd8, %rd27;
	@%p11 bra 	$L__BB2_20;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p3 bra 	$L__BB2_14;

$L__BB2_8:
	mov.u32 	%r38, 0;
	mov.u32 	%r39, %r4;
	mov.u32 	%r40, %r38;

$L__BB2_9:
	not.b32 	%r28, %r38;
	cvt.u64.u32 	%rd41, %r28;
	add.s64 	%rd42, %rd41, %rd28;
	cvt.u64.u32 	%rd10, %r39;
	shl.b64 	%rd43, %rd42, 3;
	and.b64  	%rd44, %rd43, 34359738360;
	add.s64 	%rd11, %rd3, %rd44;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd45, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd45, 0;
	@%p13 bra 	$L__BB2_11;

	div.u64 	%rd66, %rd10, %rd12;
	mul.lo.s64 	%rd46, %rd66, %rd12;
	sub.s64 	%rd67, %rd10, %rd46;
	bra.uni 	$L__BB2_12;

$L__BB2_11:
	cvt.u32.u64 	%r29, %rd12;
	cvt.u32.u64 	%r30, %rd10;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd66, %r31;
	cvt.u64.u32 	%rd67, %r33;

$L__BB2_12:
	shl.b64 	%rd47, %rd28, 3;
	add.s64 	%rd48, %rd11, %rd47;
	ld.global.u64 	%rd49, [%rd48];
	mul.lo.s64 	%rd50, %rd49, %rd67;
	cvt.u32.u64 	%r34, %rd50;
	add.s32 	%r40, %r40, %r34;
	cvt.u32.u64 	%r39, %rd66;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd51, %r38;
	setp.lt.u64 	%p14, %rd51, %rd28;
	@%p14 bra 	$L__BB2_9;

	setp.eq.s64 	%p15, %rd29, 0;
	mul.wide.u32 	%rd52, %r40, 8;
	add.s64 	%rd53, %rd2, %rd52;
	shl.b64 	%rd54, %rd8, 3;
	add.s64 	%rd55, %rd1, %rd54;
	selp.b64 	%rd56, %rd55, %rd53, %p15;
	ld.global.f64 	%fd3, [%rd56];
	fma.rn.f64 	%fd4, %fd3, %fd1, %fd2;
	st.global.f64 	[%rd55], %fd4;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p16, %rd8, %rd27;
	@%p16 bra 	$L__BB2_8;
	bra.uni 	$L__BB2_20;

$L__BB2_14:
	setp.eq.s64 	%p17, %rd29, 0;
	@%p17 bra 	$L__BB2_16;

$L__BB2_15:
	ld.global.f64 	%fd5, [%rd2];
	fma.rn.f64 	%fd6, %fd5, %fd1, %fd2;
	shl.b64 	%rd57, %rd8, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd6;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p18, %rd8, %rd27;
	@%p18 bra 	$L__BB2_15;
	bra.uni 	$L__BB2_20;

$L__BB2_16:
	shl.b64 	%rd59, %rd8, 3;
	add.s64 	%rd60, %rd1, %rd59;
	ld.global.f64 	%fd7, [%rd60];
	fma.rn.f64 	%fd8, %fd7, %fd1, %fd2;
	st.global.f64 	[%rd60], %fd8;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p19, %rd8, %rd27;
	@%p19 bra 	$L__BB2_16;

$L__BB2_20:
	ret;

}
	// .globl	affine_u8
.visible .entry affine_u8(
	.param .u64 affine_u8_param_0,
	.param .u64 affine_u8_param_1,
	.param .u64 affine_u8_param_2,
	.param .u64 affine_u8_param_3,
	.param .u64 affine_u8_param_4,
	.param .u8 affine_u8_param_5,
	.param .u8 affine_u8_param_6
)
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<15>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<67>;


	ld.param.u8 	%rs2, [affine_u8_param_6];
	ld.param.u8 	%rs1, [affine_u8_param_5];
	ld.param.u64 	%rd27, [affine_u8_param_0];
	ld.param.u64 	%rd28, [affine_u8_param_1];
	ld.param.u64 	%rd30, [affine_u8_param_2];
	ld.param.u64 	%rd29, [affine_u8_param_3];
	ld.param.u64 	%rd31, [affine_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd29;
	cvta.to.global.u64 	%rd3, %rd30;
	setp.eq.s64 	%p3, %rd28, 0;
	setp.eq.s64 	%p4, %rd30, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p23, %p2;
	@%p5 bra 	$L__BB3_5;

	mov.u64 	%rd60, 1;
	mov.u32 	%r36, 0;

$L__BB3_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd33, %r22;
	add.s64 	%rd34, %rd33, %rd28;
	shl.b64 	%rd35, %rd34, 3;
	and.b64  	%rd36, %rd35, 34359738360;
	add.s64 	%rd5, %rd3, %rd36;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB3_4;

	shl.b64 	%rd37, %rd28, 3;
	add.s64 	%rd38, %rd5, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p8, %rd60, %rd39;
	mov.pred 	%p23, 0;
	@%p8 bra 	$L__BB3_5;

$L__BB3_4:
	mul.lo.s64 	%rd60, %rd6, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd40, %r36;
	setp.lt.u64 	%p10, %rd40, %rd28;
	mov.pred 	%p23, %p2;
	@%p10 bra 	$L__BB3_2;

$L__BB3_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd8, %r4;
	@%p23 bra 	$L__BB3_17;
	bra.uni 	$L__BB3_6;

$L__BB3_17:
	setp.ge.u64 	%p20, %rd8, %rd27;
	@%p20 bra 	$L__BB3_20;

	setp.eq.s64 	%p21, %rd29, 0;
	selp.b64 	%rd24, %rd1, %rd2, %p21;
	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r35;

$L__BB3_19:
	add.s64 	%rd58, %rd24, %rd8;
	ld.global.u8 	%rs12, [%rd58];
	mul.lo.s16 	%rs13, %rs12, %rs1;
	add.s16 	%rs14, %rs13, %rs2;
	add.s64 	%rd59, %rd1, %rd8;
	st.global.u8 	[%rd59], %rs14;
	add.s32 	%r4, %r4, %r18;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p22, %rd8, %rd27;
	@%p22 bra 	$L__BB3_19;
	bra.uni 	$L__BB3_20;

$L__BB3_6:
	setp.ge.u64 	%p11, %rd8, %rd27;
	@%p11 bra 	$L__BB3_20;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p3 bra 	$L__BB3_14;

$L__BB3_8:
	mov.u32 	%r38, 0;
	mov.u32 	%r39, %r4;
	mov.u32 	%r40, %r38;

$L__BB3_9:
	not.b32 	%r28, %r38;
	cvt.u64.u32 	%rd41, %r28;
	add.s64 	%rd42, %rd41, %rd28;
	cvt.u64.u32 	%rd10, %r39;
	shl.b64 	%rd43, %rd42, 3;
	and.b64  	%rd44, %rd43, 34359738360;
	add.s64 	%rd11, %rd3, %rd44;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd45, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd45, 0;
	@%p13 bra 	$L__BB3_11;

	div.u64 	%rd62, %rd10, %rd12;
	mul.lo.s64 	%rd46, %rd62, %rd12;
	sub.s64 	%rd63, %rd10, %rd46;
	bra.uni 	$L__BB3_12;

$L__BB3_11:
	cvt.u32.u64 	%r29, %rd12;
	cvt.u32.u64 	%r30, %rd10;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd62, %r31;
	cvt.u64.u32 	%rd63, %r33;

$L__BB3_12:
	shl.b64 	%rd47, %rd28, 3;
	add.s64 	%rd48, %rd11, %rd47;
	ld.global.u64 	%rd49, [%rd48];
	mul.lo.s64 	%rd50, %rd49, %rd63;
	cvt.u32.u64 	%r34, %rd50;
	add.s32 	%r40, %r40, %r34;
	cvt.u32.u64 	%r39, %rd62;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd51, %r38;
	setp.lt.u64 	%p14, %rd51, %rd28;
	@%p14 bra 	$L__BB3_9;

	setp.eq.s64 	%p15, %rd29, 0;
	cvt.u64.u32 	%rd52, %r40;
	add.s64 	%rd53, %rd2, %rd52;
	add.s64 	%rd54, %rd1, %rd8;
	selp.b64 	%rd55, %rd54, %rd53, %p15;
	ld.global.u8 	%rs3, [%rd55];
	mul.lo.s16 	%rs4, %rs3, %rs1;
	add.s16 	%rs5, %rs4, %rs2;
	st.global.u8 	[%rd54], %rs5;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p16, %rd8, %rd27;
	@%p16 bra 	$L__BB3_8;
	bra.uni 	$L__BB3_20;

$L__BB3_14:
	setp.eq.s64 	%p17, %rd29, 0;
	@%p17 bra 	$L__BB3_16;

$L__BB3_15:
	ld.global.u8 	%rs6, [%rd2];
	mul.lo.s16 	%rs7, %rs6, %rs1;
	add.s16 	%rs8, %rs7, %rs2;
	add.s64 	%rd56, %rd1, %rd8;
	st.global.u8 	[%rd56], %rs8;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p18, %rd8, %rd27;
	@%p18 bra 	$L__BB3_15;
	bra.uni 	$L__BB3_20;

$L__BB3_16:
	add.s64 	%rd57, %rd1, %rd8;
	ld.global.u8 	%rs9, [%rd57];
	mul.lo.s16 	%rs10, %rs9, %rs1;
	add.s16 	%rs11, %rs10, %rs2;
	st.global.u8 	[%rd57], %rs11;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p19, %rd8, %rd27;
	@%p19 bra 	$L__BB3_16;

$L__BB3_20:
	ret;

}
	// .globl	affine_u32
.visible .entry affine_u32(
	.param .u64 affine_u32_param_0,
	.param .u64 affine_u32_param_1,
	.param .u64 affine_u32_param_2,
	.param .u64 affine_u32_param_3,
	.param .u64 affine_u32_param_4,
	.param .u32 affine_u32_param_5,
	.param .u32 affine_u32_param_6
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<71>;


	ld.param.u64 	%rd27, [affine_u32_param_0];
	ld.param.u64 	%rd28, [affine_u32_param_1];
	ld.param.u64 	%rd30, [affine_u32_param_2];
	ld.param.u64 	%rd29, [affine_u32_param_3];
	ld.param.u64 	%rd31, [affine_u32_param_4];
	ld.param.u32 	%r21, [affine_u32_param_5];
	ld.param.u32 	%r22, [affine_u32_param_6];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd29;
	cvta.to.global.u64 	%rd3, %rd30;
	setp.eq.s64 	%p3, %rd28, 0;
	setp.eq.s64 	%p4, %rd30, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p23, %p2;
	@%p5 bra 	$L__BB4_5;

	mov.u64 	%rd64, 1;
	mov.u32 	%r46, 0;

$L__BB4_2:
	not.b32 	%r24, %r46;
	cvt.u64.u32 	%rd33, %r24;
	add.s64 	%rd34, %rd33, %rd28;
	shl.b64 	%rd35, %rd34, 3;
	and.b64  	%rd36, %rd35, 34359738360;
	add.s64 	%rd5, %rd3, %rd36;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB4_4;

	shl.b64 	%rd37, %rd28, 3;
	add.s64 	%rd38, %rd5, %rd37;
	ld.global.u64 	%rd39, [%rd38];
	setp.ne.s64 	%p8, %rd64, %rd39;
	mov.pred 	%p23, 0;
	@%p8 bra 	$L__BB4_5;

$L__BB4_4:
	mul.lo.s64 	%rd64, %rd6, %rd64;
	add.s32 	%r46, %r46, 1;
	cvt.u64.u32 	%rd40, %r46;
	setp.lt.u64 	%p10, %rd40, %rd28;
	mov.pred 	%p23, %p2;
	@%p10 bra 	$L__BB4_2;

$L__BB4_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r4, %r25, %r3, %r26;
	cvt.u64.u32 	%rd8, %r4;
	@%p23 bra 	$L__BB4_17;
	bra.uni 	$L__BB4_6;

$L__BB4_17:
	setp.ge.u64 	%p20, %rd8, %rd27;
	@%p20 bra 	$L__BB4_20;

	setp.eq.s64 	%p21, %rd29, 0;
	selp.b64 	%rd24, %rd1, %rd2, %p21;
	mov.u32 	%r43, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r43;

$L__BB4_19:
	shl.b64 	%rd61, %rd8, 2;
	add.s64 	%rd62, %rd24, %rd61;
	ld.global.u32 	%r44, [%rd62];
	mad.lo.s32 	%r45, %r44, %r21, %r22;
	add.s64 	%rd63, %rd1, %rd61;
	st.global.u32 	[%rd63], %r45;
	add.s32 	%r4, %r4, %r18;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p22, %rd8, %rd27;
	@%p22 bra 	$L__BB4_19;
	bra.uni 	$L__BB4_20;

$L__BB4_6:
	setp.ge.u64 	%p11, %rd8, %rd27;
	@%p11 bra 	$L__BB4_20;

	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r27;
	@%p3 bra 	$L__BB4_14;

$L__BB4_8:
	mov.u32 	%r48, 0;
	mov.u32 	%r49, %r4;
	mov.u32 	%r50, %r48;

$L__BB4_9:
	not.b32 	%r30, %r48;
	cvt.u64.u32 	%rd41, %r30;
	add.s64 	%rd42, %rd41, %rd28;
	cvt.u64.u32 	%rd10, %r49;
	shl.b64 	%rd43, %rd42, 3;
	and.b64  	%rd44, %rd43, 34359738360;
	add.s64 	%rd11, %rd3, %rd44;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd45, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd45, 0;
	@%p13 bra 	$L__BB4_11;

	div.u64 	%rd66, %rd10, %rd12;
	mul.lo.s64 	%rd46, %rd66, %rd12;
	sub.s64 	%rd67, %rd10, %rd46;
	bra.uni 	$L__BB4_12;

$L__BB4_11:
	cvt.u32.u64 	%r31, %rd12;
	cvt.u32.u64 	%r32, %rd10;
	div.u32 	%r33, %r32, %r31;
	mul.lo.s32 	%r34, %r33, %r31;
	sub.s32 	%r35, %r32, %r34;
	cvt.u64.u32 	%rd66, %r33;
	cvt.u64.u32 	%rd67, %r35;

$L__BB4_12:
	shl.b64 	%rd47, %rd28, 3;
	add.s64 	%rd48, %rd11, %rd47;
	ld.global.u64 	%rd49, [%rd48];
	mul.lo.s64 	%rd50, %rd49, %rd67;
	cvt.u32.u64 	%r36, %rd50;
	add.s32 	%r50, %r50, %r36;
	cvt.u32.u64 	%r49, %rd66;
	add.s32 	%r48, %r48, 1;
	cvt.u64.u32 	%rd51, %r48;
	setp.lt.u64 	%p14, %rd51, %rd28;
	@%p14 bra 	$L__BB4_9;

	setp.eq.s64 	%p15, %rd29, 0;
	mul.wide.u32 	%rd52, %r50, 4;
	add.s64 	%rd53, %rd2, %rd52;
	shl.b64 	%rd54, %rd8, 2;
	add.s64 	%rd55, %rd1, %rd54;
	selp.b64 	%rd56, %rd55, %rd53, %p15;
	ld.global.u32 	%r37, [%rd56];
	mad.lo.s32 	%r38, %r37, %r21, %r22;
	st.global.u32 	[%rd55], %r38;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p16, %rd8, %rd27;
	@%p16 bra 	$L__BB4_8;
	bra.uni 	$L__BB4_20;

$L__BB4_14:
	setp.eq.s64 	%p17, %rd29, 0;
	@%p17 bra 	$L__BB4_16;

$L__BB4_15:
	ld.global.u32 	%r39, [%rd2];
	mad.lo.s32 	%r40, %r39, %r21, %r22;
	shl.b64 	%rd57, %rd8, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u32 	[%rd58], %r40;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p18, %rd8, %rd27;
	@%p18 bra 	$L__BB4_15;
	bra.uni 	$L__BB4_20;

$L__BB4_16:
	shl.b64 	%rd59, %rd8, 2;
	add.s64 	%rd60, %rd1, %rd59;
	ld.global.u32 	%r41, [%rd60];
	mad.lo.s32 	%r42, %r41, %r21, %r22;
	st.global.u32 	[%rd60], %r42;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p19, %rd8, %rd27;
	@%p19 bra 	$L__BB4_16;

$L__BB4_20:
	ret;

}
	// .globl	affine_i64
.visible .entry affine_i64(
	.param .u64 affine_i64_param_0,
	.param .u64 affine_i64_param_1,
	.param .u64 affine_i64_param_2,
	.param .u64 affine_i64_param_3,
	.param .u64 affine_i64_param_4,
	.param .u64 affine_i64_param_5,
	.param .u64 affine_i64_param_6
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<85>;


	ld.param.u64 	%rd27, [affine_i64_param_0];
	ld.param.u64 	%rd28, [affine_i64_param_1];
	ld.param.u64 	%rd32, [affine_i64_param_2];
	ld.param.u64 	%rd29, [affine_i64_param_3];
	ld.param.u64 	%rd33, [affine_i64_param_4];
	ld.param.u64 	%rd30, [affine_i64_param_5];
	ld.param.u64 	%rd31, [affine_i64_param_6];
	cvta.to.global.u64 	%rd1, %rd33;
	cvta.to.global.u64 	%rd2, %rd29;
	cvta.to.global.u64 	%rd3, %rd32;
	setp.eq.s64 	%p3, %rd28, 0;
	setp.eq.s64 	%p4, %rd32, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p23, %p2;
	@%p5 bra 	$L__BB5_5;

	mov.u64 	%rd78, 1;
	mov.u32 	%r36, 0;

$L__BB5_2:
	not.b32 	%r22, %r36;
	cvt.u64.u32 	%rd35, %r22;
	add.s64 	%rd36, %rd35, %rd28;
	shl.b64 	%rd37, %rd36, 3;
	and.b64  	%rd38, %rd37, 34359738360;
	add.s64 	%rd5, %rd3, %rd38;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB5_4;

	shl.b64 	%rd39, %rd28, 3;
	add.s64 	%rd40, %rd5, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	setp.ne.s64 	%p8, %rd78, %rd41;
	mov.pred 	%p23, 0;
	@%p8 bra 	$L__BB5_5;

$L__BB5_4:
	mul.lo.s64 	%rd78, %rd6, %rd78;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd42, %r36;
	setp.lt.u64 	%p10, %rd42, %rd28;
	mov.pred 	%p23, %p2;
	@%p10 bra 	$L__BB5_2;

$L__BB5_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r4, %r23, %r3, %r24;
	cvt.u64.u32 	%rd8, %r4;
	@%p23 bra 	$L__BB5_17;
	bra.uni 	$L__BB5_6;

$L__BB5_17:
	setp.ge.u64 	%p20, %rd8, %rd27;
	@%p20 bra 	$L__BB5_20;

	setp.eq.s64 	%p21, %rd29, 0;
	selp.b64 	%rd24, %rd1, %rd2, %p21;
	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r35;

$L__BB5_19:
	shl.b64 	%rd72, %rd8, 3;
	add.s64 	%rd73, %rd24, %rd72;
	ld.global.u64 	%rd74, [%rd73];
	mul.lo.s64 	%rd75, %rd74, %rd30;
	add.s64 	%rd76, %rd75, %rd31;
	add.s64 	%rd77, %rd1, %rd72;
	st.global.u64 	[%rd77], %rd76;
	add.s32 	%r4, %r4, %r18;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p22, %rd8, %rd27;
	@%p22 bra 	$L__BB5_19;
	bra.uni 	$L__BB5_20;

$L__BB5_6:
	setp.ge.u64 	%p11, %rd8, %rd27;
	@%p11 bra 	$L__BB5_20;

	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r25;
	@%p3 bra 	$L__BB5_14;

$L__BB5_8:
	mov.u32 	%r38, 0;
	mov.u32 	%r39, %r4;
	mov.u32 	%r40, %r38;

$L__BB5_9:
	not.b32 	%r28, %r38;
	cvt.u64.u32 	%rd43, %r28;
	add.s64 	%rd44, %rd43, %rd28;
	cvt.u64.u32 	%rd10, %r39;
	shl.b64 	%rd45, %rd44, 3;
	and.b64  	%rd46, %rd45, 34359738360;
	add.s64 	%rd11, %rd3, %rd46;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd47, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd47, 0;
	@%p13 bra 	$L__BB5_11;

	div.u64 	%rd80, %rd10, %rd12;
	mul.lo.s64 	%rd48, %rd80, %rd12;
	sub.s64 	%rd81, %rd10, %rd48;
	bra.uni 	$L__BB5_12;

$L__BB5_11:
	cvt.u32.u64 	%r29, %rd12;
	cvt.u32.u64 	%r30, %rd10;
	div.u32 	%r31, %r30, %r29;
	mul.lo.s32 	%r32, %r31, %r29;
	sub.s32 	%r33, %r30, %r32;
	cvt.u64.u32 	%rd80, %r31;
	cvt.u64.u32 	%rd81, %r33;

$L__BB5_12:
	shl.b64 	%rd49, %rd28, 3;
	add.s64 	%rd50, %rd11, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	mul.lo.s64 	%rd52, %rd51, %rd81;
	cvt.u32.u64 	%r34, %rd52;
	add.s32 	%r40, %r40, %r34;
	cvt.u32.u64 	%r39, %rd80;
	add.s32 	%r38, %r38, 1;
	cvt.u64.u32 	%rd53, %r38;
	setp.lt.u64 	%p14, %rd53, %rd28;
	@%p14 bra 	$L__BB5_9;

	setp.eq.s64 	%p15, %rd29, 0;
	mul.wide.u32 	%rd54, %r40, 8;
	add.s64 	%rd55, %rd2, %rd54;
	shl.b64 	%rd56, %rd8, 3;
	add.s64 	%rd57, %rd1, %rd56;
	selp.b64 	%rd58, %rd57, %rd55, %p15;
	ld.global.u64 	%rd59, [%rd58];
	mul.lo.s64 	%rd60, %rd59, %rd30;
	add.s64 	%rd61, %rd60, %rd31;
	st.global.u64 	[%rd57], %rd61;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p16, %rd8, %rd27;
	@%p16 bra 	$L__BB5_8;
	bra.uni 	$L__BB5_20;

$L__BB5_14:
	setp.eq.s64 	%p17, %rd29, 0;
	@%p17 bra 	$L__BB5_16;

$L__BB5_15:
	ld.global.u64 	%rd62, [%rd2];
	mul.lo.s64 	%rd63, %rd62, %rd30;
	add.s64 	%rd64, %rd63, %rd31;
	shl.b64 	%rd65, %rd8, 3;
	add.s64 	%rd66, %rd1, %rd65;
	st.global.u64 	[%rd66], %rd64;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p18, %rd8, %rd27;
	@%p18 bra 	$L__BB5_15;
	bra.uni 	$L__BB5_20;

$L__BB5_16:
	shl.b64 	%rd67, %rd8, 3;
	add.s64 	%rd68, %rd1, %rd67;
	ld.global.u64 	%rd69, [%rd68];
	mul.lo.s64 	%rd70, %rd69, %rd30;
	add.s64 	%rd71, %rd70, %rd31;
	st.global.u64 	[%rd68], %rd71;
	add.s32 	%r4, %r4, %r5;
	cvt.u64.u32 	%rd8, %r4;
	setp.lt.u64 	%p19, %rd8, %rd27;
	@%p19 bra 	$L__BB5_16;

$L__BB5_20:
	ret;

}

